{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "adf29506e2f669d1f973a1335c691114",
     "grade": false,
     "grade_id": "cell-5cb269cc88b84f7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h1><center>Gradient Based Optimization</center></h1>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for CS-EJ3311 - Deep Learning with Python</font></center>\n",
    "<center><font size=\"3\">24.10.-11.12.2022</font></center>\n",
    "<center><font size=\"3\">Aalto University & FiTech.io</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c341a72b79cdc4329fabf30c2b655ba3",
     "grade": false,
     "grade_id": "cell-f67d71d3c0644230",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This notebook demonstrates a simple but powerful method to find an optimal choice for the parameters (weights and biases) of an artificial neural network (ANN). The idea is to tune (adjust) the parameters according to the gradient of the average loss incurred by the ANN on a training set. This average loss is also known as the **training loss** (or training error) and defines an **objective or cost function** $f(\\mathbf{w})$ that we want to minimize.  \n",
    "\n",
    "Here we discuss a simple iterative algorithm which is called **gradient descent** (GD). GD minimizes the training loss by incrementally improving the current guess for the optimal parameters by moving a bit into the direction of the negative gradient. We will also discuss a simple variant of GD known as **stochastic gradient descent** (SGD). SGD is one of the most widely used optimization methods within deep learning.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "- understand how gradients of the loss function can be used to learn the parameters of an ANN\n",
    "\n",
    "- understand the basic idea behind stochastic gradient descent (SGD)\n",
    "\n",
    "- understand SGD components \"batch\", \"batch size\", \"learning rate\" and \"epoch\"\n",
    "\n",
    "- be aware of some advanced variants of SGD such as ADAM or RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9814153f3477999393d2b1449a6c292c",
     "grade": false,
     "grade_id": "cell-7f83fda88459b81f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Additional Reading\n",
    "\n",
    "-  Loss function chapter 1.1.5 & Gradient-based optimization chapter 2.4 of the book \"Deep Learning with Python\" by F. Chollet. \n",
    "-  Gradient descent Chapter 4 & 11 of \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by Aurélien Géron. \n",
    "\n",
    "Advanced reading:\n",
    "\n",
    "- Chapter 2.3 & 5 of \"Machine Learning: Basic Principles\" https://arxiv.org/pdf/1805.05052.pdf\n",
    "- Optimization Chapter of Deep Learning Book https://www.deeplearningbook.org/contents/optimization.html\n",
    "\n",
    "## Video materials\n",
    "\n",
    "Beginner-friendly videos:\n",
    "- 3Blue1Brown, https://www.youtube.com/watch?v=IHZwWFHWa-w\n",
    "- StatQuest, https://www.youtube.com/watch?v=sDv4f4s2SB8\n",
    "\n",
    "Videos from Andrew Ng's courses  with a brief intro to:\n",
    "- GD, https://www.youtube.com/watch?v=F6GSRDoB-Cg \n",
    "- Adam, https://youtu.be/JXQT_vxqwIs \n",
    "- RMSprop, https://youtu.be/_e-LFe_igno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92d6cf44621b6a5ae7931558439c9f31",
     "grade": false,
     "grade_id": "cell-be6ce98897a3b562",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@font-face {\n",
       "  font-family: 'Sanchez';\n",
       "  src: url('https://fonts.googleapis.com/css?family=Sanchez:400italic,400');\n",
       "}\n",
       "\n",
       "@import url('https://fonts.googleapis.com/css2?family=Sanchez&display=swap');\n",
       "\n",
       "* {\n",
       "  margin: 0;\n",
       "  padding: 0;\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "\n",
       "*,\n",
       "*:before,\n",
       "*:after {\n",
       "\tbox-sizing: inherit;\n",
       "}\n",
       "\n",
       "body {\n",
       "font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Helvetica,\n",
       "    Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\";\n",
       "}\n",
       "\n",
       ".title-container {\n",
       "  text-align: left;\n",
       "}\n",
       "\n",
       ".title {\n",
       "  font-weight: 600;\n",
       "}\n",
       "\n",
       ".subtitle {\n",
       "  margin: 10px 0px;\n",
       "  color: #888888;\n",
       "  font-size: 25px;\n",
       "  transition: all 0.5s;\n",
       "}\n",
       "\n",
       ".main-container {\n",
       "  padding: 15px;\n",
       "}\n",
       "\n",
       ".card-container {\n",
       "  display: flex;\n",
       "  flex-wrap: wrap;\n",
       "  justify-content: space-between;\n",
       "}\n",
       "\n",
       ".card {\n",
       "  margin: 20px;\n",
       "  padding: 20px;\n",
       "  width: 100%;\n",
       "  min-height: 200px;\n",
       "  display: grid;\n",
       "  grid-template-columns: 1fr 1fr 1fr;\n",
       "  gap: 10px;\n",
       "  border-radius: 10px;\n",
       "  box-shadow: 0px 6px 10px rgba(0, 0, 0, 0.25);\n",
       "  transition: all 0.5s;\n",
       "}\n",
       "\n",
       ".card.small {\n",
       "  width: 50%;\n",
       "  min-height: 100px;\n",
       "  grid-template-columns: 1fr 1fr;\n",
       "}\n",
       "\n",
       ".card:hover {\n",
       "  box-shadow: 0px 6px 10px rgba(0, 0, 0, 0.4);\n",
       "  transform: scale(1.01);\n",
       "}\n",
       "\n",
       ".card__title {\n",
       "  grid-columnn-start: 1;\n",
       "  grid-columnn-end: -1;\n",
       "  font-weight: 400;\n",
       "  color: #ffffff;\n",
       "}\n",
       "\n",
       ".test-input {\n",
       "  grid-column-start: 1;\n",
       "  grid-column-end: 2;\n",
       "  color: #40413e;\n",
       "}\n",
       "\n",
       ".test-output {\n",
       "  grid-column-start: 2;\n",
       "  grid-column-end: 3;\n",
       "  color: #40413e;\n",
       "}\n",
       "\n",
       ".test-expected-output {\n",
       "  grid-column-start: 3;\n",
       "  grid-column-end: 4;\n",
       "  color: #40413e;\n",
       "}\n",
       "\n",
       ".card-failure {\n",
       "  background: radial-gradient(#fbc1cc, #fa99b2);\n",
       "}\n",
       "\n",
       ".card-failure .card__title::before {\n",
       "    display: inline-block;\n",
       "    margin-right: 5px;\n",
       "    font-style: normal;\n",
       "    font-variant: normal;\n",
       "    text-rendering: auto;\n",
       "    -webkit-font-smoothing: antialiased;\n",
       "    font-family: \"Font Awesome 5 Free\";\n",
       "    font-weight: 900;\n",
       "    content: \"\\f057\";\n",
       "}\n",
       "\n",
       ".card-success {\n",
       "  background: radial-gradient(#60efbc, #58d5c9);\n",
       "}\n",
       "\n",
       ".card-success .card__title::before {\n",
       "    display: inline-block;\n",
       "    margin-right: 5px;\n",
       "    font-style: normal;\n",
       "    font-variant: normal;\n",
       "    text-rendering: auto;\n",
       "    -webkit-font-smoothing: antialiased;\n",
       "    font-family: \"Font Awesome 5 Free\";\n",
       "    font-weight: 900;\n",
       "    content: \"\\f058\";\n",
       "}\n",
       "\n",
       ".card-info {\n",
       "  background: radial-gradient(#1fe4f5, #3fbafe);\n",
       "}\n",
       "\n",
       "@media (max-width: 1600px) {\n",
       "  .card-container {\n",
       "    justify-content: center;\n",
       "  }\n",
       "}\n",
       "\n",
       ".code-block {\n",
       "  padding: 5px;\n",
       "  background-color: #f3f7f7;\n",
       "  color: black;\n",
       "  border-radius: 10px;\n",
       "  box-shadow: 0px 6px 10px rgba(0, 0, 0, 0.25);\n",
       "}\n",
       "\n",
       "details {\n",
       "\tfont-size: 1rem;\n",
       "\tbox-shadow: 0 10px 15px -5px rgba(0, 0, 0, 0.1),\n",
       "\t\t0 10px 10px -5px rgba(0, 0, 0, 0.04);\n",
       "\twidth: 100%;\n",
       "\tbackground: #ffffff;\n",
       "\tborder-radius: 10px;\n",
       "\tposition: relative;\n",
       "}\n",
       "\n",
       "details:hover {\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".summary-title {\n",
       "    user-select: none;\n",
       "    margin-left: 5px;\n",
       "}\n",
       "\n",
       ".summary-content {\n",
       "    border: 2px solid #0C7B89;\n",
       "    cursor: default;\n",
       "    padding: 1em;\n",
       "    font-weight: 300;\n",
       "    font-size: 15px;\n",
       "    line-height: 1.5;\n",
       "}\n",
       "\n",
       ".wrap-up {\n",
       "    border: 2px solid #B41086;\n",
       "    cursor: default;\n",
       "    padding: 20px;\n",
       "    line-height: 1.5;\n",
       "    border-radius: 20px\n",
       "}\n",
       "\n",
       ".wrap-up-title {\n",
       "    font-size:20px;\n",
       "    color: #B41086;\n",
       "    font-weight: bold\n",
       "}\n",
       "\n",
       ".wrap-up-content {\n",
       "    font-size:14px;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".info {\n",
       "    background-color: white;\n",
       "    border: 2px solid #0C7B89;\n",
       "    cursor: default;\n",
       "    padding: 10px;\n",
       "    line-height: 1.5;\n",
       "    border-radius: 20px\n",
       "}\n",
       "\n",
       ".info-title {\n",
       "    font-size: 20px;\n",
       "    color: #0C7B89;\n",
       "    font-weight: bold\n",
       "}\n",
       "\n",
       "summary {\n",
       "   color: white;\n",
       "   font-size: large;\n",
       "   font-weight: bold;\n",
       "   padding: 1em;\n",
       "   background-color: #0C7B89;\n",
       "   border-radius: 8px;\n",
       "   list-style: none;\n",
       "}\n",
       "\n",
       "details[open] summary {\n",
       "    border-radius: 8px 8px 0 0;\n",
       "}\n",
       "\n",
       "details[open] summary::before {\n",
       "  transform: rotate(90deg);\n",
       "  font-family: \"Font Awesome 5 Free\";\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       "details summary::before {\n",
       "  position: absolute;\n",
       "  will-change: transform;\n",
       "  transition: transform 300ms ease;\n",
       "  font-family: \"Font Awesome 5 Free\";\n",
       "  color: #fff;\n",
       "  font-size: 1.1rem;\n",
       "  content: \"\\f105\";\n",
       "  left: 0;\n",
       "  display: inline-block;\n",
       "  width: 1.6rem;\n",
       "  text-align: center;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       "summary:focus {\n",
       "  outline: none;\n",
       "}\n",
       "\n",
       "summary::-webkit-details-marker {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       ".ludwig {\n",
       "  position: relative;\n",
       "  padding-left: 1em;\n",
       "  border-left: 0.2em solid lighten( hsl(200, 40, 10), 40%);\n",
       "  font-family: 'Roboto', serif;\n",
       "  font-size: 1.3em;\n",
       "  line-height: 1.5em;\n",
       "  font-weight: 100;\n",
       "}\n",
       "\n",
       ".ludwig::before, .ludwig::after {\n",
       "  content: '\\201C';\n",
       "  font-family: 'Sanchez';\n",
       "  color: lighten( hsl(200, 40, 10), 40%);\n",
       "}\n",
       "\n",
       ".ludwig::after {\n",
       "  content: '\\201D';\n",
       "}\n",
       "\n",
       ".blockquote-container {\n",
       "  margin: 2em auto;\n",
       "}\n",
       "\n",
       "blockquote {\n",
       "  margin-bottom: 3em;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_styles\n",
    "\n",
    "# This MUST be the last line of this cell\n",
    "load_styles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4662e9725c358a8cebfe8614166cf9eb",
     "grade": false,
     "grade_id": "cell-b0e3481a9dd3e926",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Deep learning aims at finding a good choice for the weights (and biases) of an **artificial neural network (ANN)**. We need to define a loss function to measure how \"good\" a particular choice for these parameters is. For a given pair of predicted label value $\\hat{y}$ and true label value $y$, the loss function $L(y,\\hat{y})$ provides a measure for the error, or \"loss\", incurred in predicting the true label $y$ by $\\hat{y}$. We emphasize that the precise definition of the loss function is a design choice. In principle, the deep learning engineer is free to choose an arbitrary loss function used to guide the training of an ANN.  \n",
    "\n",
    "Some particular choices for the loss function have been proven to be useful in many applications. If the label values are numeric (like a temperature or a weight), then the **squared error** loss $ L(y,\\hat{y})=(y-\\hat{y})^2$ is a good choice for the loss function. \\\n",
    "If the label values are categories (like \"cat\" and \"dog\"), we might use  the logistic loss $L(y,\\hat{y}) = -y\\ln\\big(\\hat{p}(y=1)\\big)-(1-y)\\ln\\big(\\hat{p}(y=0)\\big)$ where $\\hat{p}$ is an estimate for the probability that label value is $y=1$ or $y=0$.\n",
    "\n",
    "To measure the quality of particular choice for the parameters of an ANN, we typically use a set of labeled data points \n",
    "\n",
    "$$\\big(\\mathbf{x}^{(1)},y^{(1)}\\big),\\ldots,\\big(\\mathbf{x}^{(m)},y^{(m)}\\big).$$\n",
    "\n",
    "We first compute the resulting predictions $\\hat{y}^{(i)}$ obtained when feeding the feature vectors $\\mathbf{x}^{(i)}$ into the ANN. Based on the predictions, we calculate the average loss (or training loss)\n",
    "\n",
    "$$ (1/m) \\big( L(y^{(1)},\\hat{y}^{(1)})+L(y^{(2)},\\hat{y}^{(2)})+\\ldots+L(y^{(m)},\\hat{y}^{(m)}) \\big).$$\n",
    "\n",
    "Note that the training loss depends on the parameters $\\mathbf{w}$ of the ANN via the predictions $\\hat{y}^{(i)}$. Indeed, the predictions $\\hat{y}^{(i)}=h^{(\\mathbf{w})}\\big(\\mathbf{x}^{(i)}\\big)$ are obtained by applying the ANN with parameters $\\mathbf{w}$ to the input feature vector $\\mathbf{x}^{(i)}$. By evaluating the training loss for different choices for the weights, we obtain a **cost or objective function** $f(\\mathbf{w})$. The objective function $f(\\mathbf{w})$ is the average loss incurred by an ANN with parameters $\\mathbf{w}$. It seems natural to learn parameters of an ANN that minimize the training loss, i.e., that solve\n",
    "\n",
    "$$ \\min_{\\mathbf{w} \\in \\mathbb{R}^{d}} f(\\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b525717c28e1bae3d7393bf8b4c77372",
     "grade": false,
     "grade_id": "cell-fd3af423ccbcda83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"info\">\n",
    "    <div  class=\"info-title\"><i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>&nbsp; Info</div><br>\n",
    "    <div class=\"wrap-up-content\">\n",
    "        Here we use following format:\n",
    "        <br>\n",
    "        <ul>\n",
    "          <li>non-bold font and lower case for scalar variables ( true label $y$, predicted label $\\hat{y}$ )</li>\n",
    "          <li>bold font and lower case for vectors ( features of a datapoint stacked into a vector $\\mathbf{x}$, ANN parameters stacked into a vector $\\mathbf{w}$ )</li>\n",
    "          <li>bold font and upper case for matrices ( feature matrix $\\mathbf{X}$ )</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e5bace7cf44f0787252a39cc04b7b2d",
     "grade": false,
     "grade_id": "cell-74b899199a3ac3b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Mean Squared Error \n",
    "\n",
    "Maybe the most widely used loss function for applications involving numeric label values $y \\in \\mathbb{R}$ is the squared error loss \n",
    "\n",
    "$$L(y,\\hat{y}) = (\\underbrace{y- \\hat{y}}_{\\mbox{prediction error}})^{2}.$$\n",
    "\n",
    "We assess the quality of a predictor $\\hat{y} = h^{(\\mathbf{w})}(\\mathbf{x})$ by the average loss incurred over a set of labeled data points (the **training set**). For the squared error, average loss is referred to as the **mean squared error (MSE)** \n",
    "\n",
    "$$ f(\\mathbf{w}) = (1/m) \\big( \\big( y^{(1)}-\\hat{y}^{(1)}\\big)^{2}+\\big( y^{(2)}-\\hat{y}^{(2)}\\big)^{2}+\\ldots+\\big( y^{(m)}-\\hat{y}^{(m)}\\big)^{2} \\big). $$\n",
    "Note that the MSE on the right hands side depends on the weight vector $\\mathbf{w}$ via the predictions $\\hat{y}^{(i)}$ obtained by applying the predictor map $h^{(\\mathbf{w})}\\big(\\mathbf{x}^{(i)}\\big)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9c97b7a9653de0fcde76c238a9aa3ea",
     "grade": false,
     "grade_id": "cell-7a19ba4e42949ab2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The shape of the loss $f(\\mathbf{w})$, viewed as a function of the weights $\\mathbf{w}$, depends on two components. First, it depends on how the predictor map depends on the weights (the structure of the ANN in deep learning context). Second, it depends on the choice of the loss function $L(y,\\hat{y})$ used to measure the loss incurred by predicting the true label value $y$ with the prediction $\\hat{y}$. \n",
    "\n",
    "The combination of linear predictor function and squared error loss $L(y,\\hat{y})=(y-\\hat{y})^{2}$ is a very popular as they result in a [convex](https://en.wikipedia.org/wiki/Convex_function) and [differentiable](https://en.wikipedia.org/wiki/Differentiable_function) loss function $f(\\mathbf{w})$. A convex function has the attractive property that any local minimum is always also a [global minimum](https://en.wikipedia.org/wiki/Maxima_and_minima#/media/File:Extrema_example_original.svg). If a convex function is also differentiable, it can be minimized by a simple but powerful algorithm which is known as **gradient descent**. \n",
    "\n",
    "<img src=\"../../../coursedata/SGD/MSELinPred.jpeg\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1926db6782080c70661c29fa9ac2d330",
     "grade": false,
     "grade_id": "cell-a24f9b508deb7119",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Deep learning methods use predictor maps represented by ANN with tunable weights. In this case, the predictor depends non-linearly on the weights. As a result, we obtain (highly) non-convex loss landscapes. Below, examples of loss function landscapes of more complicated models (neural networks) illustrate that finding a minimum of these loss functions is not a trivial task.\n",
    "\n",
    "<img src=\"../../../coursedata/SGD/NNloss.png\" width=500/>\n",
    "\n",
    "\n",
    "<center><a href=\"https://www.cs.umd.edu/~tomg/projects/landscapes/\">image source</a></center>\n",
    "<center><a href=\"https://arxiv.org/abs/1712.09913/\">original paper</a></center>\n",
    "\n",
    "Here you can find more examples of visualizations for loss functions obtained from representing a predictor map using ANN:\n",
    "\n",
    "[3D visualization of NN loss functions](http://www.telesens.co/loss-landscape-viz/viewer.html) \\\n",
    "[3D animation of NN loss functions](https://www.youtube.com/watch?time_continue=32&v=aq3oA6jSGro&feature=emb_logo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5668064dd9463000d27d8d07ea20072b",
     "grade": false,
     "grade_id": "cell-bc3add6f815d8e8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Gradient Descent \n",
    "\n",
    "We will now introduce a simple algorithm that allows to find (a good approximation to) the optimal parameter vector $\\mathbf{w}_{\\rm opt}$ for a predictor map $h^{(\\mathbf{w})}(\\mathbf{x})$. The optimum weight vector should result in the smallest possible loss  \n",
    "\n",
    "\\begin{align} \n",
    "f(\\mathbf{w}_{\\rm opt}) = \\min_{\\mathbf{w} \\in \\mathbb{R}^{d}} f(\\mathbf{w}) \\mbox{ with } f(\\mathbf{w})& = (1/m) \\sum_{i=1}^{m} \\big(y^{(i)} - \\hat{y}^{(i)} \\big)^{2} \\nonumber \\\\ \n",
    "& =(1/m) \\sum_{i=1}^{m} \\big(y^{(i)} - h^{(\\mathbf{w})}\\big(\\mathbf{x}^{(i)}\\big) \\big)^{2}. \\end{align}\n",
    "\n",
    "**Gradient descent (GD)** constructs a sequence of parameter vectors $\\mathbf{w}^{(0)},\\mathbf{w}^{(1)},\\ldots$ such that the loss values $f\\big(\\mathbf{w}^{(0)}\\big),f\\big(\\mathbf{w}^{(1)}\\big),\\ldots$ tends toward the minimum loss. GD is an iterative algorithm that gradually improves the current guess (approximation) $\\mathbf{w}^{(k)}$ for the optimum weight vector.  \n",
    "\n",
    "There are many different strategies for choosing the first (or initial) guess $\\mathbf{w}^{(0)}$. One simple approach is to choose the initial weights randomly. Given the current weight vector $\\mathbf{w}^{(k)}$, how does GD know in which \"direction\" to go to find a better parameter vector $\\mathbf{w}^{(k+1)}$? Mathematics, or [calcululs](https://en.wikipedia.org/wiki/Differential_calculus) to be specific, tells us that this direction is precisely the opposite of the gradient $\\nabla f(\\mathbf{w})$. More precisely, for a small step size, the steepest descent is towards the opposite direction of the [gradient](https://en.wikipedia.org/wiki/Derivative). We can think of [GD](https://en.wikipedia.org/wiki/Gradient_descent) as imitating a hiker who takes a sequence of (small) steps downhill.\n",
    "\n",
    "<img src=\"../../../coursedata/SGD/GradientHiker.jpeg\" width=500/>\n",
    "\n",
    "Given the downhill direction $- \\nabla f\\big(\\mathbf{w}^{(k)}\\big)$ at the current estimate $\\mathbf{w}^{(k)}$, we take a step \n",
    "\n",
    "$$\\mbox{(Gradient Step)} \\quad \\underbrace{\\mathbf{w}^{(k+1)}}_{\\mbox{new guess}} = \\underbrace{\\mathbf{w}^{(k)}}_{\\mbox{current guess}} - \\underbrace{\\alpha}_{\\mbox{step size}} \\nabla f\\big(\\mathbf{w}^{(k)}\\big).$$ \n",
    "\n",
    "Here, we used a tuning parameter $\\alpha>0$ which adjusts the step size for the steep downhill. We will refer to this parameter as **learning rate**. This name is due to the fact that choosing a larger value for $\\alpha$ tends to speed up the progress of GD to reach the optimum weight vector. Thus, increasing the value of $\\alpha$ tends to speed up the learning of a good weight vector for a predictor map $h^{(\\mathbf{w})}$. \n",
    "\n",
    "The GD algorithm requires the specification of a suitable learning rate  $\\alpha$ and initial guess $\\mathbf{w}^{(0)}$ and then repeating the gradient step for a sufficient number of iterations. One possible stopping criterion is to use a fixed number of iterations which might be dictated by constraints on processing duration we grant for GD (computing time costs money, [see here](https://aws.amazon.com/emr/pricing/)). \n",
    "\n",
    "Another option is to monitor the loss function and stop if consecutive iterates do not result in any significant decrease. Similarly, we could monitor the validation loss which is obtained by applying the predictor map using the current GD iterate $\\mathbf{w}^{(k)}$ to validation data which is different from the training data used to define the training loss. \n",
    "\n",
    "A key challenge in the use of GD is to find a good choice for the learning rate $\\alpha$. If the learning rate is too small (left plot below), the GD steps make too little progress and thus require an excessive number of iterations to get close to the optimum weight vector. Conversely, if the learning rate is too high (right plot below), it is possible that GD iterates $\\mathbf{w}^{(k)}$ will \"overshoot\" the minimum and climb up the loss function on the other side of the minimum (GD diverges). \n",
    "\n",
    "<img src=\"../../../coursedata/SGD/lrate.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23844f355103c4ec0b6b0a526c3ed672",
     "grade": false,
     "grade_id": "cell-924816fce21eb6a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The figure below shows how GD adapts the parameter vector $\\mathbf{w}$ of a linear predictor $h^{(\\mathbf{w})}(\\mathbf{x})= \\mathbf{w}^{T} \\mathbf{x}$ to better fit the labeled data points (left) resulting in a smaller MSE (right). Note that after around $200$ iterations, gradient descent found weight vectors resulting in an almost minimum MSE. The additional iterations (beyond $200$) are (in some sense) a waste of computation as they do not decrease the MSE significantly. \n",
    "\n",
    "<img src=\"../../../coursedata/SGD/plainGD.gif\" width=\"750\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cb9b089c352872028a1d77e68121d1d",
     "grade": false,
     "grade_id": "cell-140d9675efb76a99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To develop some intuition for the functioning of GD, let us work out the gradient update for the special case of linear predictor maps $h^{(\\mathbf{w})}(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$. Here, we can find a closed-form expression for the gradient: \n",
    "\n",
    "\\begin{align} \n",
    "\\nabla f\\big( \\mathbf{w}^{(k)} \\big)= - (2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big) \\big). \\end{align}\n",
    "\n",
    "The gradient update of GD then becomes, in turn, \n",
    "\n",
    "\\begin{align} \n",
    "\\mathbf{w}^{(k+1)} & = \\mathbf{w}^{(k)} + {\\alpha}\\,(2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big)\\nonumber \\\\ \n",
    " & = \\mathbf{w}^{(k)} + {\\alpha}\\,(2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big( y^{(i)} - \\hat{y}^{(i)} \\big) \n",
    ". \\end{align}\n",
    "\n",
    "Note that the gradient update involves the computation of the predictions $\\hat{y}^{(i)} = \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)}$. \n",
    "\n",
    "After the forward pass, the weight vector $\\mathbf{w}^{(k)}$ is updated by a weighted combination of the feature vectors $\\mathbf{x}^{(i)}$. The weight for the $i$th feature vector $\\mathbf{x}^{(i)}$ is given by the prediction error $ \\big( y^{(i)}$ - $\\hat{y}^{(i)} \\big)$ incurred by the current parameter vector for that data point. Thus, the gradient update puts more emphasis (larger weight) on those data points $\\big(\\mathbf{x}^{(i)},y^{(i)}\\big)$ which are not well predicted using the current weight vector $\\mathbf{w}^{(k)}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76f5e48941d30253569bc3e9ecec88c5",
     "grade": false,
     "grade_id": "cell-872656208f52d61b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Many machine learning and deep learning Python libraries, such as `sklearn` and `keras`, provide ready-to-use gradient-based optimization algorithms. However, it is instructive to implement our own simplified gradient descent algorithm for learning purposes. In this simple case, we have 100 data points (samples) that are described by only one feature `x`, and a label `y`. \\\n",
    "We need to find the optimal linear predictor `y_pred = weight * x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c13002524902ef5cdb06d8878ef989fc",
     "grade": false,
     "grade_id": "cell-d062ee24a84dc848",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import Python libraries\n",
    "import numpy as np                                 # library for numerical arrays (vectors, matrices, tensors)\n",
    "import matplotlib.pyplot as plt                    # library providing tools for plotting data \n",
    "from sklearn import preprocessing                  # function for pre-processing input data\n",
    "from sklearn.linear_model import LinearRegression  # sklearn class for fitting linear predictor \n",
    "from sklearn.datasets import make_regression       # function to generate a random regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e150520543c34294260d55cc7c66a10b",
     "grade": false,
     "grade_id": "cell-7903eaf0c4e0eb78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>STUDENT TASK 2.1.</b> GRADIENT DESCENT. </h3>\n",
    "        \n",
    "Your task is to implement the gradient descent algorithm for a dataset of size $m$, where $i$th datapoint is characterized by only one feature ${x}^{(i)}$. Feature values for all $m$ data points are stored in a numpy array `x` of shape (m,1). Labels of datapoints are stored in numpy array `y` of shape (m,1).\n",
    "\n",
    "Implement GD algorithm in two steps:\n",
    "    \n",
    "- implement gradient \"step\" or one iteration of GD in Python function `gradient_step_onefeature()`\n",
    "- define Python function `GD()`, which uses `gradient_step_onefeature()` and iterates the gradient step epochs times.\n",
    "  \n",
    "**Implementation note.**\n",
    "\n",
    "You can use for-loop to compute training loss and gradient for each datapoint, but we advise to use vectorized implementation where you use NumPy arrays (vectors) `x` and `y` and dot product operation for computing predictions and gradient.    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef6fe2c5fa83c66f281c8748e725c9a4",
     "grade": false,
     "grade_id": "cell-d23608ab77205d2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Hints</span>\n",
    "    </summary>\n",
    "    <div class=\"summary-content\">\n",
    "\n",
    "To implement the `gradient_step_onefeature()` function you will need to perform following steps:\n",
    "    \n",
    "1. Compute predictions for all data points, given the current weight $w$, and store predictions in numpy array `y_hat` of shape (m,1).\\\n",
    "The prediction for $i$th data point is:\n",
    "\\begin{align} \n",
    "\\hat{y}^{(i)} =w{x}^{(i)}\n",
    "\\end{align} \n",
    "\n",
    "\n",
    "2. Compute the MSE training loss, given the true labels $y^{(i)}$ and the predictions $\\hat{y}^{(i)}$ for $m$ data points. Store the result in a variable `MSE` (scalar).\\\n",
    "The training loss for $i$th data point is:\n",
    "\\begin{align} \n",
    "f\\big(w \\big)= \\big(y^{(i)} - \\hat{y}^{(i)} \\big)^{2}\n",
    "\\end{align}    \n",
    "To obtain MSE loss over $m$ data points you need to compute **average** of training losses over $m$ datapoints.\n",
    "\n",
    "\n",
    "3. Compute the **average** gradient of the loss function $f\\big(w \\big)$.\\\n",
    "The gradient of loss function for $i$th data point given current weight ${w}^{(k)}$ is:\n",
    "\\begin{align} \n",
    "\\nabla f\\big(w^{(k)} \\big)= - 2x\\big(y^{(i)} - \\hat{y}^{(i)} \\big)\n",
    "\\end{align} \n",
    "or\n",
    "\\begin{align} \n",
    "\\nabla f\\big(w^{(k)} \\big)= - 2x\\big(y^{(i)} - w^{(k)}{x}^{(i)} \\big)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "4. Update the weight - change the weight values to the opposite direction from the gradient \n",
    "    \n",
    "\\begin{align} \n",
    "w^{(k+1)}= w^{(k)} - \\alpha\\nabla f\\big(w^{(k)} \\big)\n",
    "\\end{align}   \n",
    "     </div>    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ba4271d3c25da9a065128f0e4651943",
     "grade": false,
     "grade_id": "cell-788861738364f21c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_step_onefeature(x,y,weight,lrate):\n",
    "    \n",
    "    '''\n",
    "    Function for performing gradient descent step for linear predictor and MSE as loss function.\n",
    "    \n",
    "    The inputs to the function are:\n",
    "     - numpy array with feature values x of shape (m,1)\n",
    "     - numpy array with labels y of shape (m,1)\n",
    "     - scalar value `weight`, which is the weight used for computing the prediction\n",
    "     - scalar value `lrate`, which is a coefficient alpha used during weight update (learning rate)\n",
    "\n",
    "    The function will return a new weight guess (updated weight value) and the current MSE value.   \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # performing the Gradient Step:\n",
    "    # 1. compute predictions, given the weight vector w\n",
    "    y_hat = weight * x\n",
    "    \n",
    "    # 2. compute MSE loss\n",
    "    MSE = np.mean(np.square(y-y_hat))\n",
    "    \n",
    "    # 3. compute the average gradient of the loss function\n",
    "    m = x.shape[0]\n",
    "    grad_w = 2 * (np.sum(np.mean(x * (y-y_hat))))\n",
    "    \n",
    "    # 4. update the weights\n",
    "    weight = weight + lrate*grad_w \n",
    "\n",
    "    return weight, MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0585ae7137489c5ff4c8308c6443c8f6",
     "grade": false,
     "grade_id": "cell-bae380ea380e69a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonpickle in /opt/software/lib/python3.9/site-packages (2.2.0)\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"main-container\">\n",
       "    <div class=\"title-container\">\n",
       "        <h1 class=\"title\">Congratulations!</h1>\n",
       "        <h3 class=\"subtitle\">You have passed the test cases.</h3>\n",
       "\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 1</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.37454012]\n",
       " [0.95071431]]</p>\n",
       " <p>y = [[0.73199394]\n",
       " [0.59865848]]</p>\n",
       " <p>lrate = 1.5</p>\n",
       " <p>weight = 0.1</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(1.208350726356283, 0.36799281738590045)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(1.208350726356283, 0.36799281738590045)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 2</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.15599452]\n",
       " [0.05808361]\n",
       " [0.86617615]\n",
       " [0.60111501]]</p>\n",
       " <p>y = [[0.70807258]\n",
       " [0.02058449]\n",
       " [0.96990985]\n",
       " [0.83244264]]</p>\n",
       " <p>lrate = 1.0</p>\n",
       " <p>weight = 0.1</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(0.7691133861459882, 0.46410948255628437)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(0.7691133861459882, 0.46410948255628437)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 3</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.18182497]\n",
       " [0.18340451]\n",
       " [0.30424224]\n",
       " [0.52475643]\n",
       " [0.43194502]\n",
       " [0.29122914]]</p>\n",
       " <p>y = [[0.61185289]\n",
       " [0.13949386]\n",
       " [0.29214465]\n",
       " [0.36636184]\n",
       " [0.45606998]\n",
       " [0.78517596]]</p>\n",
       " <p>lrate = 1.5</p>\n",
       " <p>weight = 1.0</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(1.06880479562059, 0.07610880618246248)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(1.06880479562059, 0.07610880618246248)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 4</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.98323089]\n",
       " [0.46676289]\n",
       " [0.85994041]\n",
       " [0.68030754]\n",
       " [0.45049925]\n",
       " [0.01326496]\n",
       " [0.94220176]\n",
       " [0.56328822]]</p>\n",
       " <p>y = [[0.3854165 ]\n",
       " [0.01596625]\n",
       " [0.23089383]\n",
       " [0.24102547]\n",
       " [0.68326352]\n",
       " [0.60999666]\n",
       " [0.83319491]\n",
       " [0.17336465]]</p>\n",
       " <p>lrate = 0.5</p>\n",
       " <p>weight = 0.1</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(0.2960020655316332, 0.1839832320288022)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(0.2960020655316332, 0.1839832320288022)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 5</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.66252228]\n",
       " [0.31171108]\n",
       " [0.52006802]\n",
       " [0.54671028]\n",
       " [0.18485446]\n",
       " [0.96958463]\n",
       " [0.77513282]\n",
       " [0.93949894]\n",
       " [0.89482735]\n",
       " [0.59789998]]</p>\n",
       " <p>y = [[0.92187424]\n",
       " [0.0884925 ]\n",
       " [0.19598286]\n",
       " [0.04522729]\n",
       " [0.32533033]\n",
       " [0.38867729]\n",
       " [0.27134903]\n",
       " [0.82873751]\n",
       " [0.35675333]\n",
       " [0.28093451]]</p>\n",
       " <p>lrate = 0.5</p>\n",
       " <p>weight = 0.01</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(0.2730909997031229, 0.20685906270480103)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(0.2730909997031229, 0.20685906270480103)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your solution \n",
    "!pip install jsonpickle\n",
    "from round02 import test_gradient_step_one_feature\n",
    "\n",
    "test_gradient_step_one_feature(gradient_step_onefeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0bcc19e561a125e0a5ca90ef356ffbd",
     "grade": true,
     "grade_id": "cell-850ad195e8f09930",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f276ebd59b1ee6fd24966f7b27c18c58",
     "grade": false,
     "grade_id": "cell-988a0c530bc19ce6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's use the `gradient_step_onefeature()` function and define a new function, `GD_onefeature()`, which will repeat the gradient step for a fixed amount of times (epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74dfb69028725becd9d28fdb10773798",
     "grade": false,
     "grade_id": "cell-4a3575bb4c4ff610",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def GD_onefeature(x,y,epochs,lrate):  \n",
    "    \n",
    "    '''\n",
    "    Function for performing gradient descent for linear predictor and MSE as loss function.\n",
    "    The helper function `gradient_step_onefeature` performs gradient step for a dataset of size `m`, \n",
    "    where each datapoint has only one feature. \n",
    "    \n",
    "    The inputs to the function `GD_onefeature()` are:\n",
    "    - numpy array with the feature values x of shape (m,1)\n",
    "    - numpy array with the labels y of shape (m,1)\n",
    "    - scalar value `epochs`, which is the number of epochs \n",
    "    - scalar value `lrate`, which is the coefficient alpha used during weight update (learning rate)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # initialize weight vector randomly\n",
    "    np.random.seed(42)\n",
    "    weight = np.random.rand()    \n",
    "    # create lists to store the loss and weight values \n",
    "    losses = []\n",
    "    weights = []\n",
    "     \n",
    "    for i in range(epochs):\n",
    "        # run the gradient step for the whole data set\n",
    "        weight, MSE = gradient_step_onefeature(x,y,weight,lrate)\n",
    "        # store current weight and training loss \n",
    "        weights.append(weight)\n",
    "        losses.append(MSE)\n",
    "                       \n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d4c7169d9763df875589e02c7a8db24",
     "grade": false,
     "grade_id": "cell-f786b3942c413385",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code snippet below uses the sklearn.dataset `make_regression` function to generate data points with features and labels. The feature and label values are obtained from random generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c176d1f16587c068f5a22fde05970c3f",
     "grade": false,
     "grade_id": "cell-663b4c32a2e5d018",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix x (n_samples, n_features):  (100, 1)\n",
      "Shape of label vector y (n_samples, 1):  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# generate dataset for regression problem\n",
    "\n",
    "x, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=42) \n",
    "y = y.reshape(-1,1)\n",
    "x = preprocessing.scale(x)\n",
    "\n",
    "print(\"Shape of feature matrix x (n_samples, n_features): \", x.shape)\n",
    "print(\"Shape of label vector y (n_samples, 1): \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebf44e180f4fc9c04d711164a292c0e8",
     "grade": false,
     "grade_id": "cell-c9f775200ef813cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We used the sklearn `preprocessing` module ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn-preprocessing-scale)) to scale our features `x`. Learn [here](https://www.youtube.com/watch?v=r5E2X1JdHAU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=20), why it is useful to normalize the data when applying the gradient descent algorithm. \n",
    "\n",
    "Below we use our simple GD algorithm and plot results in two graphs: Loss vs weight values and Loss vs epochs. You can see that training loss decreased dramatically within first 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dee54ab5b390e696de768d21ca3ac45a",
     "grade": false,
     "grade_id": "cell-b4ce18fd464b6ab3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnAAAADVCAYAAADXXc3XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAncUlEQVR4nO3de5zcdX3v8dd7Z3eTWSC7SUhCsgETIKRyESKRonjBWwPqkUirDdaKradRi623R06Jnla0JxUbL61tpSdeCvYIFCUGDl4iAopahC4ECBdjwkXIJiThsiSQZZPsfvrH77cwWWY3O5vdmd/89v18PH6Pmfn+LvPJMPvjM9+rIgIzMzMzqx8NtQ7AzMzMzCrjBM7MzMyszjiBMzMzM6szTuDMzMzM6owTODMzM7M64wTOzMzMrM401jqAajr88MNjzpw5tQ7DzKrk9ttvfzwiptU6jtHg+5fZ+DPUPWxcJXBz5syho6Oj1mGYWZVI+m2tYxgtvn+ZjT9D3cPchGpmZmZWZ5zAmZmZmdWZcdWEOhxr1nWycu0GtnR1M6utyLJF81m8oL3WYZmZmZk9zwlciTXrOlm+ej3de3sB6OzqZvnq9QBO4szMzCwz3IRaYuXaDc8nb/269/aycu2GGkVkZlaZx55+jmXfuYs7H+2qdShmNoacwJXY0tVdUbmZWdZ07+3lO7dv5qHHn6l1KGY2hpzAlZjVVqyo3Mwsa1qaCwDs3tN7gCPNrJ45gSuxbNF8ik2F/cqKTQWWLZpfo4jMzCpTTBO4bidwZrnmQQwl+gcqeBSqmdWrlibXwJmNB07gBli8oN0Jm5nVrcZCA82NDU7gzHKuqk2oko6UdJOk+yXdK+kjafkUSddL2pg+Ti45Z7mkTZI2SFpUUn6qpPXpvq9IUjX/LWZmWdXSXGD3nn21DsPMxlC1+8DtAz4RES8FTgcukHQ8cCFwQ0TMA25IX5PuWwKcAJwFfFVSfye1S4ClwLx0O6ua/xAzs6xqaSq4Bs4s56qawEXE1oi4I32+C7gfaAfOAS5LD7sMWJw+Pwe4MiJ6IuIhYBNwmqSZwKSIuCUiAvhWyTlmZuNasbngQQxmOVezUaiS5gALgFuBGRGxFZIkD5ieHtYOPFpy2ua0rD19PrDczGzca2ludBOqWc7VJIGTdChwNfDRiNg51KFlymKI8nLvtVRSh6SOHTt2VB6smVmNjPT+lfSBcw2cWZ5VPYGT1ESSvH07IlanxdvSZlHSx+1p+WbgyJLTZwNb0vLZZcpfJCJWRcTCiFg4bdq00fuHmJmNsZHev5zAmeVftUehCvgGcH9EfKlk17XA+enz84FrSsqXSJogaS7JYIXb0mbWXZJOT6/53pJzzMzGNTehmuVfteeBOwP4Y2C9pDvTsk8CFwNXSXo/8AjwToCIuFfSVcB9JCNYL4iI/p+VHwIuBYrAD9PNzGzc8yAGs/yragIXEb+gfP81gDcOcs4KYEWZ8g7gxNGLzswsH1qaC+ze6wTOLM+8FqqZWc4U3QfOLPecwJmZ5cwhzY3s2dfHvt6+WodiZmPECZyZWc60NKcL2rsZ1Sy3nMCZmeVMMU3gPJDBLL+cwJmZ5czzNXBO4MxyywmcmVnOFJuSCQY8F5xZfjmBMzPLmRY3oZrlnhM4M7Oc6U/gnnUCZ5ZbTuDMzHKmpTlpQu12E6pZbjmBMzPLGQ9iMMs/J3BmZjnjBM4s/5zAmZnljOeBM8u/qiZwkr4pabuke0rK/kPSnen2sKQ70/I5krpL9v1ryTmnSlovaZOkr0hSNf8dZmZZ1t8HzjVwZvnVWOX3uxT4Z+Bb/QUR8Yf9zyV9EXi65PgHIuKUMte5BFgK/Ar4AXAW8MPRD9fMrP4UGkRzY4PngTPLsarWwEXEzcCT5faltWjvAq4Y6hqSZgKTIuKWiAiSZHDxKIdqZlbXDmkuuAbOLMey1AfuNcC2iNhYUjZX0jpJP5P0mrSsHdhccszmtKwsSUsldUjq2LFjx+hHbWY2Rg7m/tXS3OgEzizHspTAncf+tW9bgaMiYgHwceBySZOAcv3dYrCLRsSqiFgYEQunTZs2qgGbmY2lg7l/FZsLdO91E6pZXlW7D1xZkhqBc4FT+8siogfoSZ/fLukB4DiSGrfZJafPBrZUL1ozs+xrcROqWa5lpQbuTcCvI+L5plFJ0yQV0udHA/OAByNiK7BL0ulpv7n3AtfUImgzs6wqNhXY3eMEziyvqj2NyBXALcB8SZslvT/dtYQXD154LXC3pLuA7wIfjIj+ARAfAr4ObAIewCNQzcz209JcYLebUM1yq6pNqBFx3iDl7ytTdjVw9SDHdwAnjmpwZmY50jKhkd1P7q51GGY2RrLShGpmZqOopanglRjMcswJnJlZDnkQg1m+OYEzM8uhYnOja+DMcswJnJlZDrU0F9jT28fe3r5ah2JmY8AJnJlZDrU0FwAvaG+WV07gzMxyqJgmcG5GNcsnJ3BmZjl0SHMyS9TuPZ4LziyPnMCZmeVQ0U2oZrnmBM7MLIfcB84s35zAmZnl0AsJnJtQzfLICZyZWQ4Vm5I+cB7EYJZPTuDMzHLITahm+VbVBE7SNyVtl3RPSdlFkjol3ZlubynZt1zSJkkbJC0qKT9V0vp031ckqZr/DjOzrGuZkCZwe53AmeVRtWvgLgXOKlP+5Yg4Jd1+ACDpeGAJcEJ6zlclFdLjLwGWAvPSrdw1zczGrZbm/iZU94Ezy6OqJnARcTPw5DAPPwe4MiJ6IuIhYBNwmqSZwKSIuCUiAvgWsHhMAjYzq1PFpuT37rM9roEzy6Os9IH7sKS70ybWyWlZO/BoyTGb07L29PnA8rIkLZXUIaljx44dox23mdmYOZj7V6FBTGhsoNtNqGa5lIUE7hLgGOAUYCvwxbS8XL+2GKK8rIhYFRELI2LhtGnTDjLU7FqzrpMzLr6RuRd+nzMuvpE16zprHZKZHaSDvX+1NBc8jYhZTjXWOoCI2Nb/XNLXgOvSl5uBI0sOnQ1sSctnlykft9as62T56vXP/9Lu7Opm+er1ACxeMGjlpJnlXEtzo0ehmuVUzWvg0j5t/d4B9I9QvRZYImmCpLkkgxVui4itwC5Jp6ejT98LXFPVoDNm5doNL2om6d7by8q1G2oUkZllQUtzgWd7XANnlkdVrYGTdAVwJnC4pM3Ap4EzJZ1C0gz6MPABgIi4V9JVwH3APuCCiOjPUj5EMqK1CPww3catLV3dFZWb2fjQWmxiZ7cTOLM8qmoCFxHnlSn+xhDHrwBWlCnvAE4cxdDq2qy2Ip1lkrVZbcUaRGNmWdFabGLr08/VOgwzGwM1b0K1g7ds0fznpwzoV2wqsGzR/BpFZGZZ0NrSxNPde2sdhpmNgZoPYrCD1z9QYeXaDWzp6mZWW5Fli+Z7AIPZONdadAJnlldO4HJi8YJ2J2xmtp+2YjPP9Oxjb28fTQU3uJjlif+izcxyqrWY/Ebf6Vo4s9wZlQRO0tTRuI6ZmY2etpZmADejmuVQRQmcpD+TtKzk9UnpdCDb0+Vejhj1CM3MbERai00AdDmBM8udSmvg/gIona/iS0AX8FGgFfjsqERlZmYHrbUlSeBcA2eWP5UOYjgK+DWApFbgdcDiiPiBpCeAz41yfGZmNkL9NXBP73YCZ5Y3ldbAFYC+9PmrSVZP+Gn6+lFg+uiEZWZmB6ut6Bo4s7yqNIHbCLw1fb4E+M+I2J2+ngU8OVqBmZnZwZnU3wfONXBmuVNpE+oXgH+XdD4wGXhnyb7XA3ePVmBmZnZwmgoNHNJccA2cWQ5VVAMXEZeT9Hv7HPD6iFhdsnsb8E9DnS/pm5K2S7qnpGylpF9LulvS9yS1peVzJHVLujPd/rXknFMlrZe0SdJXJKmSf4eZ2XjR1tLsBM4shyqeBy4ifhERX4yImweUfzoifnCA0y8FzhpQdj1wYkS8DPgNsLxk3wMRcUq6fbCk/BJgKTAv3QZe08zMSJpRn+7eU+swzGyUVToP3Kskva3k9VRJV6S1YV+QVBjq/DTpe3JA2Y8jYl/68lfA7APEMBOYFBG3REQA3wIWV/LvMDMbL9q8HqpZLlVaA3cxcGrJ65XAW0hqzj4EfPIg4/lT4Iclr+dKWifpZ5Jek5a1A5tLjtmclpmZ2QCtxSYPYjDLoUoTuJcCHQCSmoA/AD4WEb8PfAp490gDkfQpYB/w7bRoK3BURCwAPg5cLmkSUK6/Wwxx3aXpKhEdO3bsGGl4ZmZVNxr3r7YW18CZ5VGlCdyhwM70+WnAIcB16es7SCb6rVg6qvVtwB+lzaJERE9EPJE+vx14ADiOpMattJl1NrBlsGtHxKqIWBgRC6dNmzaS8MzMamI07l+txSYvpWWWQ5UmcJ3Ayenzs4F7ImJ7+noysLvsWUOQdBbwV8DbS+aUQ9K0/j51ko4mGazwYERsBXZJOj0dffpe4JpK39fMbDxobWliz74+ntvbW+tQzGwUVToP3BXA30k6k6Tv26dL9r2cZKLfQUm6AjgTOFzS5vT85cAE4Pp0NpBfpSNOXwt8VtI+oBf4YET0D4D4EMmI1iJJn7nSfnNmZpZqLVmNYWLTkOPMzKyOVJrAXQQ8B5xOMqDhyyX7Tga+M9TJEXFemeJvDHLs1cDVg+zrAE48cLhmZuNbW7EZSFZjmDFpYo2jMbPRUlECFxG9wIpB9i0ejYDMzGz0tHo9VLNcqrQGDgBJJ5KsyDAFeAK4OSLuGfosMzOrtraW/vVQPZmvWZ5UlMBJaiTpe3Ye+0/nEZIuB96X1tKZmVkGuAbOLJ8qHYX6aeBdwN8Ac0kGEcxNX/9h+mhmZhnR2uIEziyPKm1CfQ/wtxFR2g/ut8CKdMqPP2H/kalmZlZDhzY30iAncGZ5U2kN3CzglkH2/We638zMMqKhQUzyclpmuVNpArcFOGOQfa9iiBURzMysNrygvVn+VNqE+m3gU5L60udbgSOAJSRroX5+dMMzM7OD1eoEzix3RjKR79HAZ9Ln/QRcnpabmVmGtLY0ez1Us5ypdCLffcC7Ja0gWepqCvAk8DOS/m/rgJeNdpBWn9as62Tl2g1s6epmVluRZYvms3hBe63DMht3WotNPPpkxUtVm1mGjWgi34i4F7i3tEzSS4ETRiMoq39r1nWyfPV6utMFtDu7ulm+ej2AkzizKmsrNnkiX7OcqXQQg9mwrFy74fnkrV/33l5Wrt1Qo4jMxq+2lqQPXF9f1DoUMxslVU3gJH1T0nZJ95SUTZF0vaSN6ePkkn3LJW2StEHSopLyUyWtT/d9RZIGvpfV1pau7orKzWzsTDtsAn0BTzzrWjizvKh2DdylwFkDyi4EboiIecAN6WskHU8yuvWE9JyvppMFA1wCLAXmpdvAa1qNzWorVlRuZmNnxqSJADz29HM1jsTMRssBEzhJRw9nI5lOZEgRcTPJoIdS5wCXpc8vAxaXlF8ZET0R8RCwCThN0kxgUkTcEhEBfKvkHMuIZYvmU2wq7FdWbCqwbNH8GkVkNn7NbE0TuJ1O4MzyYjiDGDYBw+k4oWEeN9CMiNgKEBFbJU1Py9uBX5Uctzkt25s+H1huGdI/UMGjUM1q74hJTuDM8mY4CdyfjHkU5ZXr1xZDlJe/iLSUpLmVo446anQis2FZvKDdCZvZQRit+9fUQydQaBCPPe0+qGZ5ccAELiIuO9AxB2mbpJlp7dtMYHtavhk4suS42SRLdW1Onw8sLysiVgGrABYuXOghWGZWN0br/lVoEDMOm8BjT/eMWmxmVltZmEbkWuD89Pn5wDUl5UskTZA0l2Swwm1pc+suSaeno0/fW3KOmZmVMaN1ItvchGqWGyOayHekJF0BnAkcLmkz8GngYuAqSe8HHgHeCclkwZKuAu4D9gEXRET/xGIfIhnRWgR+mG5mZjaIIyZN5DfbdtU6DDMbJVVN4CLivEF2vXGQ41cAK8qUdwAnjmJoZma5NmPSRH6+8fFah2FmoyQLTahmZjbGZrZO5Jmefex6zovam+WBEzgzs3HgiHQuOPeDM8sHJ3BmZuPAC6sxeCSqWR44gTMzGwc8ma9ZvjiBMzMbB/qbUD2Zr1k+OIEzMxsHJjYVaGtpcg2cWU44gTMzGyeOmDTRfeDMcsIJnJnZODFjkldjMMsLJ3BmZuPEzNaJbH3aCZxZHjiBMzMbJ2ZMmsgTz/awt7ev1qGY2UGq6lJaZtWwZl0nK9duYEtXN7PaiixbNJ/FC9prHZZZzR3ROpEI2L6rh/a2Yq3DMbOD4Bo4y5U16zpZvno9nV3dBNDZ1c3y1etZs66z1qGZ1dzzc8F5KhGzupeJBE7SfEl3lmw7JX1U0kWSOkvK31JyznJJmyRtkLSolvFbdqxcu4Huvb37lXXv7WXl2g01isgsO2a2JQlcZ5f7wZnVu0w0oUbEBuAUAEkFoBP4HvAnwJcj4gulx0s6HlgCnADMAn4i6biI2P//3DbubOkqX7MwWLnZeDJn6iE0CDZtf6bWoZjZQcpEDdwAbwQeiIjfDnHMOcCVEdETEQ8Bm4DTqhKdZdqsQfr1DFZuNp5MbCpw1JQWNm3fVetQzOwgZTGBWwJcUfL6w5LulvRNSZPTsnbg0ZJjNqdlNs4tWzSfYlNhv7JiU4Fli+bXKCKzbJk34zB+s801cGb1LlMJnKRm4O3Ad9KiS4BjSJpXtwJf7D+0zOkxyDWXSuqQ1LFjx47RDdgyZ/GCdj537km0txUR0N5W5HPnnuRRqFaXxuL+NW/6oTz8+LPs2eepRMzqWSb6wJU4G7gjIrYB9D8CSPoacF36cjNwZMl5s4Et5S4YEauAVQALFy4sm+RZvixe0O6EzXJhLO5fx804jH19wcNPPMtxMw4bjUuaWQ1kqgYOOI+S5lNJM0v2vQO4J31+LbBE0gRJc4F5wG1Vi9LMrE7Nm3EoAL/Z5n5wZvUsMzVwklqANwMfKCn+e0mnkDSPPty/LyLulXQVcB+wD7jAI1DNzA7smGmH0iDY6H5wZnUtMwlcROwGpg4o++Mhjl8BrBjruMzM8qR/JOpGj0Q1q2tZa0I1M7Mxdux0j0Q1q3dO4MzMxpnjZngkqlm9cwJnZjbOlI5ENbP6lJk+cGb1YM26Tlau3cCWrm5mtRVZtmi+pyyxunPs9BdGonoqEbP65ATObJjWrOtk+er1dO9NBjx3dnWzfPV6ACdxVleOnX4o8khUs7rmJlSzYVq5dsPzyVu/7r29rFy7oUYRmY3MxKYCc6Yewn1bd9Y6FDMbISdwZsO0pau7onKzLHvFnMnc+uAT9PZ5gRqzeuQEzmyYZrUVKyo3y7Izjj2cnc/t474troUzq0dO4MyGadmi+RSbCvuVFZsKLFs0v0YRmY3cK49O5k3/5QOP1zgSMxsJJ3Bmw7R4QTufO/ck2tuKCGhvK/K5c0/yAAarS9MnTWTe9EP55SYncGb1yKNQzSqweEG7EzbLjTOOPZwr/+sR9uzro7nRv+fN6on/Ys1qZM26Ts64+EbmXvh9zrj4Rtas66x1SDbOvPKYqTy3t491jzxV61DMrEKZSeAkPSxpvaQ7JXWkZVMkXS9pY/o4ueT45ZI2SdogaVHtIjerXP+ccp1d3QQvzCnnJM6q6fSjp9Ig+OUDT9Q6FDOrUGYSuNTrI+KUiFiYvr4QuCEi5gE3pK+RdDywBDgBOAv4qqRCuQuaZZHnlLMsaC02cWJ7K7d4IINZ3claAjfQOcBl6fPLgMUl5VdGRE9EPARsAk6rfnhmI+M55SwrXn3s4ax7pIsdu3pqHYqZVSBLCVwAP5Z0u6SladmMiNgKkD5OT8vbgUdLzt2clr2IpKWSOiR17NixY4xCN6uM55Sz4ajG/evcl89mX1/wndsfPfDBZpYZWUrgzoiIlwNnAxdIeu0Qx6pMWdnpxCNiVUQsjIiF06ZNG404zQ6a55Sz4ajG/evY6Ydy+tFTuPzWR+jzqgxmdSMzCVxEbEkftwPfI2kS3SZpJkD6uD09fDNwZMnps4Et1YvW7OAc7JxyHsFqo+mPfvclbH6qm5s3upXCrF5kYh44SYcADRGxK33+e8BngWuB84GL08dr0lOuBS6X9CVgFjAPuK3qgZsdhJHOKdc/grV/EET/CNb+a5pVatEJRzD1kGa+fesjnDl/+oFPMLOay0oN3AzgF5LuIknEvh8RPyJJ3N4saSPw5vQ1EXEvcBVwH/Aj4IKI6C17ZbOc8QhWG23NjQ286xVHcsP92zyQxqxOZCKBi4gHI+LkdDshIlak5U9ExBsjYl76+GTJOSsi4piImB8RP6xd9GbV5RGsNhbefdpRFBrE53/061qHYmbDkIkEzsyGbyQjWN1nzg7kyCkt/PmZx3LNnVu4acP2A59gZjXlBM6szlQ6gtWrPthw/fnrj+GYaYfwv793D7v37Kt1OGY2BCdwZnWm0hGslfSZc03d+DahscDFv/8yOru6uejae4nwtCJmWZWJUahmVplKRrAOt8+cR7cawCvmTOHDrz+Wf75pExObCnzm7ScglZt608xqyQmcWc7NaivSWSaJG9hnbqiauv4Ebs26Tlau3cCWrm5mtRVZtmi+k7sc+sTvHcee3j5W3fwgfRH89duOZ0Kjl5s2yxIncGY5t2zR/P1q1qB8n7kD1dQNVUMHvCixK1fmZK8+SGL52b+DgP9784Pc+uCTfP4PXsbLj5pc69DMLKXx1Mdh4cKF0dHRUeswzKpuODVnZ1x8Y9mauva2Ir+88A2D7m8rNtGzr2+/BLGpQSDY2xv7lR06sZGu3XtpLTYhsd/zp3bvRQyyJt4gHr74rUPul3R7RCys4JKZVav7100btvOp1evZuvM5zjxuGu85/SWcOX86hQY3q5qNtaHuYU7gzAx4cQ0bJDV1/QMk5l74/YqSq2oZKolzAjc6nunZx9dufpDLb3uEHbt6aC028cqjp/K7R09h/hGHcdyMw5h6SLP7ypmNsqHuYW5CNTPghYEKg9XUDdaXzvLv0AmNfOzNx/HhNxzLT+7bxk0btvOLjY/zo3sfe/6YCY0NHNE6kamHNNPW0sykiY0UmxtpaS4wobGB5sYGmgoNNBVEoaGBgqDQICTRIKGk0jZ9TF70p4OlieHAFNE5o9WT5sYG3vayWaNyLSdwZva8oUa3DtaXbmJTA0/t3lutEK2GmgoNnH3STM4+aSYRwfZdPfxm2y42bnuGx3Y+x5aubp7avYftu55j0/Z97N7Ty+49+9izr499fVmsvzWrrsktTU7gzKy6BquhA16U2JXrA2f5IokZkyYyY9JEXjNv2gGP7+0L9vYmiVxvb9AbQW9fEAR9fRAEEUkfyIjk+UADyyKTjfpmg2sYxSpjJ3BmNmxD1dANNQq1tdjEs3v2OaEbxwoNotDgqUjMRksmEjhJRwLfAo4A+oBVEfGPki4C/gzYkR76yYj4QXrOcuD9QC/wlxGxtuqBmxkweGJXWlY6ErZao1DNzPIqEwkcsA/4RETcIekw4HZJ16f7vhwRXyg9WNLxwBLgBGAW8BNJx0XE/rOQmllmVLJ6hJmZDS0Ta6FGxNaIuCN9vgu4HxjqTn8OcGVE9ETEQ8Am4LSxj9TMzMys9jKRwJWSNAdYANyaFn1Y0t2SvimpfxrwduDRktM2M0jCJ2mppA5JHTt27Ch3iJlZJvn+ZWaDyVQCJ+lQ4GrgoxGxE7gEOAY4BdgKfLH/0DKnl+06ExGrImJhRCycNu3AI6XMzLLC9y8zG0xW+sAhqYkkeft2RKwGiIhtJfu/BlyXvtwMHFly+mxgy4He4/bbb39c0m9HKeTDgcdH6VoHIytxQHZicRz7G89xvKTK7zdmRnD/ysp/90rVa9xQv7HXa9yQ/9gHvYdlYiktJdNsXwY8GREfLSmfGRFb0+cfA343IpZIOgG4nKTf2yzgBmBeNQcxSOrIwhI9WYkDshOL43AcVr+fd73GDfUbe73GDeM79qzUwJ0B/DGwXtKdadkngfMknULSPPow8AGAiLhX0lXAfSQjWC/wCFQzMzMbLzKRwEXELyjfr+0HQ5yzAlgxZkGZmZmZZVSmBjHUmVW1DiCVlTggO7E4jv05jvGpXj/veo0b6jf2eo0bxnHsmegDZ2ZmZmbD5xo4MzMzszrjBG4Y0kmEt0u6p6RsiqTrJW1MHycPdY0xjOMiSZ2S7ky3t1QhjiMl3STpfkn3SvpIWl7Vz2SIOKr6mUiaKOk2SXelcXwmLa/25zFYHFX/jqTvW5C0TtJ16euq/82MR5LOkrRB0iZJF9Y6nqFk5V4yUvX6HZfUJum7kn6dfvavrIfYJX0s/Z7cI+mK9J6XybgrzRskLU//ZjdIWjSc93ACNzyXAmcNKLsQuCEi5pFMY1KNG2W5OCBZL/aUdBt04Mco6l+79qXA6cAFStanrfZnMlgcUN3PpAd4Q0ScTDLp9FmSTqf6n8dgcUD1vyMAHyFZFq9fLf5mxhVJBeBfgLOB40lG8h8/9Fk1lZV7yUjV63f8H4EfRcTvACeT/BsyHbukduAvgYURcSJQIFkTPatxX8ow8wbtv777WcBX07/lITmBG4aIuBl4ckDxOSRz15E+Lq5RHFU3xNq1Vf1MRrCG7ljFERHxTPqyKd2C6n8eg8VRdZJmA28Fvl5SXPW/mXHoNGBTRDwYEXuAK0k+90zKyr1kJOr1Oy5pEvBa4BsAEbEnIrqog9hJZs4oSmoEWkgm8M9k3BXmDSNa390J3MjN6J9kOH2cXsNYyq0XWxXaf+3amn0mGt4aumP5/gUlcxhuB66PiJp8HoPEAdX/jvwD8L+AvpKyLP3N5NWw14nOmqzcSyrwD9Tnd/xoYAfwb2nz79clHULGY4+ITuALwCMkS2s+HRE/JuNxDzBYrCP6u3UCV/8GWy92zOnFa9fWRJk4qv6ZRERvRJxCsqzbaZJOHOv3rCCOqn4ekt4GbI+I28fyfaysYa8TnSVZuZcMV51/xxuBlwOXRMQC4Fmy0+w4qPSH5znAXJIVmA6R9J7aRjVqRvR36wRu5LZJmgnJkl8kNR5VFxHb0v9p9wFfYxjVrqNBZdaupQafSbk4avWZpO/dBfyUpB9Dzb4jpXHU4PM4A3i7pIdJmvDeIOn/kZG/mZwb0TrRtZSVe0mF6vk7vhnYXFI7/12ShC7rsb8JeCgidkTEXmA18CqyH3epwWId0d+tE7iRuxY4P31+PnBNLYLo/zKk3gHcM9ixo/ieIuk/cX9EfKlkV1U/k8HiqPZnImmapLb0eZHkRvNrqv95lI2j2p9HRCyPiNkRMYekY+6NEfEeMvI3k3P/BcyTNFdSM8nnf22NYxpUVu4llarn73hEPAY8Kml+WvRGkmUpsx77I8DpklrS780bSfpMZj3uUoPFei2wRNIESXOBecBtB7xaRHg7wAZcQdL0tJckU34/MJVkFMnG9HFKjeL4d2A9cHf6JZhZhTheTVK9ezdwZ7q9pdqfyRBxVPUzAV4GrEvf7x7gb9Lyan8eg8VR9e9ISUxnAtfV4vMYr1v6N/Ab4AHgU7WO5wCxZuJecpD/hrr7jpN0p+hIP/c1wOR6iB34DMmP43vS+9qErMY9yP+vB40V+FT6N7sBOHs47+GVGMzMzMzqjJtQzczMzOqMEzgzMzOzOuMEzszMzKzOOIEzMzMzqzNO4MzMzMzqjBM4yyVJF0ka0RBrST+V9IthHLdY0sdH8h5mZtUg6eF0kmHLGSdwlldfB145xu+xGHACZ2ZmVddY6wDMxkJEbCaZPNHMzCx3XANnNSNpoaSQ9OqSsr9Iy/5PSdm8tOwt6eu5kr4taYekHkl3SnrHgGu/qAk1XWrqCkk7JT0l6d8kvT299pll4nuTpDsk7ZZ0j6TFJfsuJVkKpT09P9J1Ec1snJJ0sqRr0/tLt6RfSnpNyf5LJW2W9CpJ/yXpubSJ8y/KXOs0ST+R9IykZyXdIOlF6xhLep2k6yU9nR53l6T3lzluiaT702M6Su+7Vp+cwFkt3QF0AW8oKXsD0F2mrBf4uaQjgVuBk4GPAW9Pr3O1pLcf4P1WA2cDy0nWL9wL/NMgxx4D/CPwJeBckiVRvivp2HT/3wI/AHaQNNW+kmSdUTMbhyS9HPhPYArwZ8DvA08AP5F0asmhk4D/AC4j6YbxU+Arkt5Xcq2XAT8jWeLqfcB70/N+JunkkuPOIVmSqRn4AHAO8E3gJQPCew3wCeCvgT8ECsB1/WsnW52q9Xph3sb3RrKY703p8wbgSeCLJMnVoWn5lcCv0uffIEmapg64zvXAnSWvL0q+3s+//j2SNRffNeC8a9PyM0vKfpq+/7ySsukkSeQnS8ouBTbX+jP05s1b7TeSROp+oLmkrJCWrUlfX5reb5YMOPd64Lfw/PKW3yX5cdtWcsyk9P64On0t4GGSNU0bhojrYeApYHJJ2cI0jnfX+nPzNvLNNXBWazcBr5Q0kWSB5Tbg74Eekl+NkCwWfWP6/CySmq+nJTX2b8Ba4GRJkwZ5n9NJErDvDSj/7iDHb4yIjf0vImI7sB04atj/MjMbFyQVgdcB3wH6Su5LAn4CvLbk8F7g6gGXuJLk3tKevn4tcF1EdPUfEBE7SX5wvi4tmk9S0/b1iOg7QIi3RMRTJa/Xp4++n9UxD2KwWrsRmAC8ClgA3BUR29JpPF4v6RFgBkmiB0lN2HvTrZypwM4y5TOBpyJi74DybYNc58kyZT3AxMH+IWY2bk0hqW3763R7EUn9FSZD3YfaSQZfTSHptjHQYyTNqpDc62B4g7X2u59FRI8k8P2srjmBs1pbDzxO0s9tAS/UtN0IvAt4FNgD/DItfwL4OfD5Qa63ZZDyrcBkSU0Dbp4zRh66mRmQNHf2Af8CfKvcARHRlyZNQ92HOtPHJ4EjylzmCF5Ixh5PH9vLHGfjgBM4q6mICEk/A94MvBT4arrrRuBzJLVpt0bE7rT8RyQDBu6NiO4K3upXJL+Q3wFcVVL+zoMIvwcoHsT5ZpYDEfGspJ+TDK664wBNmgWSAQ5XlpQtAR7hhQTuZ8BbJR0WEbsAJB0G/A+SProAvyHp3/Y/Ja2KiBFNXG71ywmcZcGNJL9ce0lq1yAZWboTeD3w2ZJj/wa4DbhZ0j+T3MAmAycCR0fEn5Z7g4j4cdosu0rS4cAm4A9IbriQ/Hqu1H3AFEkfIulI/FxErD/AOWaWTx8HbgbWSvoGSa3/4cDLgUJEXJgetwv4+/Q+tBE4D3gT8L6SJOxvgbcBN0j6PMmAg78CWkjvh+mP34+SjK6/UdK/kgzweikwPSI+Pcb/XqsxD2KwLOjv39aRdtQl/QV784D9RMQjJCOo7gL+jmT01iUkHXv7m18Hcy5JDd7nSWrhJvJCf5WnRxD310l+Rf8dSVL5/0dwDTPLgYi4A3gFSTePrwA/JpmK6CReuJdB8sN0Cck8kteQ/Ej9SERcVnKtu0kGb+0kmW7k34FngNdFxF0lx11D0noByQj9a4GlJD9sLefkWlcbzyT9C8k8S1MioqfG4ZhZjqUTgL8pImbXOharf25CtXEjnSizFbiXZOLLs4APAiudvJmZWT1xAmfjybPAR0lWWZgAPAR8ElhZw5jMzMwq5iZUMzMzszrjQQxmZmZmdcYJnJmZmVmdcQJnZmZmVmecwJmZmZnVGSdwZmZmZnXGCZyZmZlZnflvi7eHVURPeJEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set epoches and learning rate\n",
    "epochs = 100\n",
    "lrate = 0.1\n",
    "\n",
    "# store results\n",
    "(weights, losses) = GD_onefeature(x,y,epochs,lrate)\n",
    "\n",
    "# plot loss and weight values\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,3), sharey=True)\n",
    "\n",
    "# Loss vs weights plot\n",
    "ax[0].scatter(weights, losses)\n",
    "ax[0].set_xlabel(\"weight\", fontsize=16)\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "\n",
    "# Loss vs epoch plot\n",
    "ax[1].plot(range(epochs), losses)\n",
    "ax[1].set_xlabel(\"epoch\", fontsize=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95743c646cfb17a9580e9433f1376d44",
     "grade": false,
     "grade_id": "cell-d745d2e5ff5696e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "   <h3><b>STUDENT TASK 2.2.</b> GRADIENT DESCENT - LEARNING RATE. </h3>\n",
    "\n",
    "Your task is to try out different learning rates for the GD implementation provided in the function `GD_onefeature()`. For each value of the learning rates in the list `lrates` run the function `GD_onefeature()` that returns the sequence of weights $w^{(1)},w^{(2)},\\ldots$ and corresponding loss values $f\\big(w^{(1)}\\big),f\\big(w^{(2)}\\big)$. Store these sequences in the lists `weights_list` and `loss_list`, respectively. For each learning rate, depict the trajectory of GD given by the points $\\big(w^{(k)},f\\big(w^{(k)}\\big)\\big)$, index by the GD iteration number $k$, in a plot with horizontal axis representing the weight values and vertical axis representing the loss function values (see plot below for reference). <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "debf0c331a16daa293e5d34ffd5de620",
     "grade": false,
     "grade_id": "cell-564973a8d0e96814",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    " <img src=\"../../../coursedata/SGD/STlrate.png\" width=460/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0183f6158dec4885afc2e91c4590e17e",
     "grade": false,
     "grade_id": "cell-811a8a4d8714a5ce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 100\n",
    "lrates = [0.001, 0.01, 0.1, 0.9]\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "weights_list = []\n",
    "loss_list = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "for lrate in lrates:\n",
    "    # use GD_onefeature function (GD_onefeature(x, y, epochs, lrate))\n",
    "    # to get weights & loss values for a certain lrate\n",
    "    # append these values to weights_list & loss_list\n",
    "    weights, losses = GD_onefeature(x, y, epochs, lrate)\n",
    "    weights_list.append(weights)\n",
    "    loss_list.append(losses)\n",
    "\n",
    "# plot results\n",
    "for i,lrate in enumerate(lrates[:4]):\n",
    "    plt.plot(weights_list[i], loss_list[i], label=f\"lrate{lrate}\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.xlabel(\"weight\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df53aef81d8a6bb0e14941aaac6174fb",
     "grade": false,
     "grade_id": "cell-aa358a025eb64e94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "assert len(weights_list)==len(lrates), \"Length of list `weights_list` should be equal to length of `lrates`\"\n",
    "assert len(weights_list[0])==epochs, \"Length of list `weights_list[0]` should be equal to epoch number\"\n",
    "assert len(loss_list)==len(lrates), \"Length of list `loss_list` should be equal to length of `lrates`\"\n",
    "assert len(loss_list[0])==epochs, \"Length of list `loss_list[0]` should be equal to epoch number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e359448a35587078d6beeab740568d97",
     "grade": true,
     "grade_id": "cell-6bffb19cd2e7f648",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f011875b1492f7e34df7b7b52e073031",
     "grade": true,
     "grade_id": "cell-13c46b047c6751a7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e71d634aca6fa4bb87b555c049292cfc",
     "grade": false,
     "grade_id": "cell-8e6ed71ea6242596",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Based on the graph, which learning rate values do you think are suboptimal? What will happen if learning rate `lrate= 1` is used?\n",
    "\n",
    "Below we plot data points and linear predictors with weights computed at 1st, 50th and 100th epochs for each learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30d8ebe13687e6a6474f74bb43bc3eee",
     "grade": false,
     "grade_id": "cell-3a7b8b2316971726",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,4, sharey=True, figsize=(15,4))\n",
    "\n",
    "for i in range(len(lrates)):\n",
    "        x_grid = np.linspace(x.min(),x.max(),100)     # x-axis values for plotting\n",
    "        y_pred_1 = weights_list[i][0]*x_grid          # get weight computed at epoch 1 and compute predictions\n",
    "        y_pred_50 = weights_list[i][49]*x_grid        # epoch 50\n",
    "        y_pred_100 = weights_list[i][99]*x_grid       # epoch 100\n",
    "        \n",
    "        ax[i].scatter(x,y) # plot data points\n",
    "        ax[i].plot(x_grid, y_pred_1, label=\"epoch 1\", c='k')      # plot predictor with weight at epoch 1\n",
    "        ax[i].plot(x_grid, y_pred_50, label=\"epoch 50\", c='g')    # plot predictor with weight at epoch 50\n",
    "        ax[i].plot(x_grid, y_pred_100, label=\"epoch 100\", c='r')  # plot predictor with weight at epoch 100\n",
    "        \n",
    "        # remove top and right subplot's frames \n",
    "        ax[i].spines['top'].set_visible(False)\n",
    "        ax[i].spines['right'].set_visible(False)\n",
    "        # set subplot's title\n",
    "        ax[i].set_title(\"lrate = \"+str(lrates[i]), fontsize=18)\n",
    "\n",
    "ax[3].legend()\n",
    "plt.ylim(-150,150)      \n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35b9b874705956e93e3dc9df22d9337b",
     "grade": false,
     "grade_id": "cell-21024b817ac1e1a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St3'></a>\n",
    "<div class=\" alert alert-success\">\n",
    "    <h3><b>STUDENT TASK 2.3. (OPTIONAL, not graded) . </b> GRADIENT DESCENT. </h3> \n",
    "    \n",
    "This is an <strong>optional (not graded)</strong> task where you will try vectorized implementation of GD for data points with many features. You will need the knowledge of matrix multiplication for completing the task.\n",
    "    \n",
    "    \n",
    "Your task is to write Gradient Descent implementation for a dataset of size $m$, where $i$th datapoint is characterized by $n$ features stored in feature vector $\\mathbf{x}^{(i)}$.\n",
    "\n",
    "The feature vectors of each datapoint are stored in matrix $X$ of shape $(m,n)$. The true labels of datapoints are stored in label vector $\\mathbf{y}$ of shape (m,1). \n",
    "\n",
    "You need to implement similar steps as previously, but in this case the loss function is:\n",
    "\n",
    "\\begin{align} \n",
    "f\\big( \\mathbf{w}\\big)= (1/m) \\sum_{i=1}^{m}\\big(y^{(i)} - \\mathbf{w}^{T} \\mathbf{x}^{(i)} \\big)^2\n",
    "\\end{align}\n",
    "    \n",
    "    \n",
    "and the gradient:\n",
    "    \n",
    "\\begin{align} \n",
    "\\nabla f\\big( \\mathbf{w}^{(k)} \\big)= - (2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big) \\big)\n",
    "\\end{align}\n",
    "    \n",
    "where $\\mathbf{x}^{(i)}$ and  $\\mathbf{w}$ are arrays of shape $(n,1)$ and $n$ is a number of features.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb820d5df15be1f77c131785121bceaa",
     "grade": false,
     "grade_id": "cell-94ac1f19a0bf58bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Hints</span>\n",
    "    </summary>\n",
    "    <div class=\"summary-content\">\n",
    "\n",
    "To implement the vectorized GD, you can use following formulas:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "  \\hat{y} &= X \\times w \\\\\n",
    "  \\\\\n",
    "  MSE &= \\frac{1}{m}  \\Sigma_{i=1}^m{(y - \\hat{y})^2} \\\\\n",
    "  \\\\\n",
    "  \\nabla f\\big( \\mathbf{w} \\big) &= - \\frac{2}{m} X^T \\times (y - \\hat{y}) \\\\\n",
    "  \\\\\n",
    "  w &= w - \\alpha \\nabla f\\big( \\mathbf{w} \\big)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where: <br>\n",
    " $\\times$ represents the matrix multiplication (`@` operator in numpy) and $T$ is a transpose operator.\n",
    "     </div>    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d21bbf82933c26861efe824a1b37fc78",
     "grade": false,
     "grade_id": "cell-fe84e9067c0aa837",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_step(X, y, weight, lrate):\n",
    "    \n",
    "    '''\n",
    "    Function for performing gradient step with MSE loss function.\n",
    "      \n",
    "     The inputs to the function are:\n",
    "    - numpy array (matrix) with feature values X of shape (m,n)\n",
    "    - numpy array with labels y of shape (m,1)\n",
    "    - numpy array `weight` of shape (n,1), which is the weight used for computing prediction\n",
    "    - scalar value `lrate`, which is a coefficient alpha used during weight update\n",
    "\n",
    "    The function will return a new weight guess (updated weight value) and current MSE value.   \n",
    "    \n",
    "    '''\n",
    "# YOUR CODE HERE\n",
    "    \n",
    "    # performing Gradient Step:\n",
    "\n",
    "    # 1. Compute predictions, given the feature matrix X of shape (m,n) and weight vector w of shape (n,1).\n",
    "    #    Predictions should be stored in an array `y_hat` of shape (m,1).\n",
    "    y_hat = np.array([x @ weight for x in X])\n",
    "    \n",
    "    \n",
    "    # 2. compute MSE loss\n",
    "    m = X.shape[0]\n",
    "    MSE = np.mean(np.square(y-y_hat))\n",
    "    \n",
    "    # 3. compute average gradient of loss function\n",
    "    gradient = - 2/m * np.transpose(X)@(y-y_hat)\n",
    "    \n",
    "    # 4. update the weights\n",
    "    weight = weight - lrate * gradient\n",
    "    \n",
    "    return weight, MSE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96c07b6e23cd04afe79aa96be2351902",
     "grade": false,
     "grade_id": "cell-f7aa818e7be52c49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"main-container\">\n",
       "    <div class=\"title-container\">\n",
       "        <h1 class=\"title\">Congratulations!</h1>\n",
       "        <h3 class=\"subtitle\">You have passed the test cases.</h3>\n",
       "\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 1</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.37454012]\n",
       " [0.95071431]]</p>\n",
       " <p>y = [[0.73199394]\n",
       " [0.59865848]]</p>\n",
       " <p>lrate = 1.5</p>\n",
       " <p>weight = [[0.88587027]]</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(array([[0.76338549]]), 0.10973857268936617)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(array([[0.76338549]]), 0.10973857268936617)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 2</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.44583275 0.09997492]\n",
       " [0.45924889 0.33370861]\n",
       " [0.14286682 0.65088847]\n",
       " [0.05641158 0.72199877]]</p>\n",
       " <p>y = [[9.38552709e-01]\n",
       " [7.78765841e-04]\n",
       " [9.92211559e-01]\n",
       " [6.17481510e-01]]</p>\n",
       " <p>lrate = 1.0</p>\n",
       " <p>weight = [[0.98518458]\n",
       " [0.74867616]]</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(array([[0.94533991],\n",
       "       [0.7790699 ]]), 0.20133926039094008)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(array([[0.94533991],\n",
       "       [0.7790699 ]]), 0.20133926039094008)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 3</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.52475643 0.43194502 0.29122914]\n",
       " [0.61185289 0.13949386 0.29214465]\n",
       " [0.36636184 0.45606998 0.78517596]\n",
       " [0.19967378 0.51423444 0.59241457]\n",
       " [0.04645041 0.60754485 0.17052412]\n",
       " [0.06505159 0.94888554 0.96563203]]</p>\n",
       " <p>y = [[0.80839735]\n",
       " [0.30461377]\n",
       " [0.09767211]\n",
       " [0.68423303]\n",
       " [0.44015249]\n",
       " [0.12203823]]</p>\n",
       " <p>lrate = 0.5</p>\n",
       " <p>weight = [[0.61447138]\n",
       " [0.75710909]\n",
       " [0.5840864 ]]</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(array([[0.50123374],\n",
       "       [0.45771577],\n",
       "       [0.2301886 ]]), 0.4135551017439119)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(array([[0.50123374],\n",
       "       [0.45771577],\n",
       "       [0.2301886 ]]), 0.4135551017439119)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 4</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[0.83319491 0.17336465 0.39106061 0.18223609]\n",
       " [0.75536141 0.42515587 0.20794166 0.56770033]\n",
       " [0.03131329 0.84228477 0.44975413 0.39515024]\n",
       " [0.92665887 0.727272   0.32654077 0.57044397]\n",
       " [0.52083426 0.96117202 0.84453385 0.74732011]\n",
       " [0.53969213 0.58675117 0.96525531 0.60703425]\n",
       " [0.27599918 0.29627351 0.16526694 0.01563641]\n",
       " [0.42340148 0.39488152 0.29348817 0.01407982]]</p>\n",
       " <p>y = [[0.1988424 ]\n",
       " [0.71134195]\n",
       " [0.79017554]\n",
       " [0.60595997]\n",
       " [0.92630088]\n",
       " [0.65107703]\n",
       " [0.91495968]\n",
       " [0.85003858]]</p>\n",
       " <p>lrate = 1.0</p>\n",
       " <p>weight = [[0.58729168]\n",
       " [0.40729189]\n",
       " [0.32446768]\n",
       " [0.29054007]]</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(array([[0.32101012],\n",
       "       [0.2678811 ],\n",
       "       [0.1401994 ],\n",
       "       [0.08534322]]), 0.1582869051162567)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(array([[0.32101012],\n",
       "       [0.2678811 ],\n",
       "       [0.1401994 ],\n",
       "       [0.08534322]]), 0.15828690511625673)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "         <div class=\"card-container\">\n",
       "             <div class=\"card card-success\">\n",
       "                 <h2 class=\"card__title\">Test Case 5</h2>\n",
       "                 <div class=\"test-input\">\n",
       "                     <p>Input</p>\n",
       "                     <div class=\"code-block\">\n",
       " <p>x = [[9.54101165e-02 3.70818252e-01 6.68841253e-01 6.65922357e-01\n",
       "  5.91297788e-01]\n",
       " [2.74721793e-01 5.61243426e-01 3.82926875e-01 9.71712095e-01\n",
       "  8.48913824e-01]\n",
       " [7.21729521e-01 2.35984920e-01 2.56068323e-01 4.04335895e-02\n",
       "  7.10662890e-01]\n",
       " [1.10890821e-01 4.39336502e-01 2.01719202e-01 8.95763596e-01\n",
       "  4.75370223e-01]\n",
       " [5.63275572e-01 6.95516086e-01 1.39331454e-01 6.04417379e-01\n",
       "  5.39841091e-01]\n",
       " [2.03061225e-01 9.42853571e-01 5.98865466e-01 6.94784933e-01\n",
       "  8.80467839e-01]\n",
       " [6.24354048e-01 2.95633686e-01 1.05494260e-01 4.56534570e-01\n",
       "  2.18440437e-01]\n",
       " [4.16509948e-01 8.83280259e-01 3.24345021e-01 1.22087955e-01\n",
       "  3.56297838e-01]\n",
       " [9.06828442e-01 2.72132249e-01 6.47690121e-01 5.20376995e-04\n",
       "  3.52568856e-01]\n",
       " [3.04781258e-01 1.64655853e-01 5.34089419e-01 4.84829971e-01\n",
       "  6.92436033e-01]]</p>\n",
       " <p>y = [[0.26941233]\n",
       " [0.24412552]\n",
       " [0.16829104]\n",
       " [0.21876422]\n",
       " [0.558102  ]\n",
       " [0.40383617]\n",
       " [0.06489225]\n",
       " [0.25391541]\n",
       " [0.24687606]\n",
       " [0.69630427]]</p>\n",
       " <p>lrate = 1.5</p>\n",
       " <p>weight = [[0.52445939]\n",
       " [0.44418605]\n",
       " [0.51889415]\n",
       " [0.69212143]\n",
       " [0.88640562]]</p>       \n",
       "        </div>\n",
       "                 </div>\n",
       "                 <div class=\"test-output\">\n",
       "                     <p>Your Output</p>\n",
       "                     <div class=\"code-block\">(array([[-0.84824641],\n",
       "       [-1.37067022],\n",
       "       [-0.88088183],\n",
       "       [-1.22399141],\n",
       "       [-1.23349799]]), 1.4645206429978428)</div>\n",
       "                 </div>\n",
       "                 <div class=\"test-expected-output\">\n",
       "                     <p>Expected Output</p>\n",
       "                     <div class=\"code-block\">(array([[-0.84824641],\n",
       "       [-1.37067022],\n",
       "       [-0.88088183],\n",
       "       [-1.22399141],\n",
       "       [-1.23349799]]), 1.4645206429978432)</div>\n",
       "                 </div>\n",
       "             </div>\n",
       "         </div>\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your solution \n",
    "from round02 import test_gradient_step\n",
    "\n",
    "test_gradient_step(gradient_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da51a8f415fc9cd0d2580293dda2a4ba",
     "grade": false,
     "grade_id": "cell-47e93370eaaec0a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's use the completed `gradient_step()` function and define the `GD()` function that will repeat the gradient step for fixed amounts of times (epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "885fa8424dfcdd9ee9def071afc33dfd",
     "grade": false,
     "grade_id": "cell-6e70b0a0a2d5117d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def GD(X,y,epochs,lrate):  \n",
    "    '''\n",
    "    Function for performing gradient descent for linear predictor and MSE as loss function.\n",
    "    The helper function `gradient_step` performs gradient step for dataset of size `m`, \n",
    "    where each datapoint has `n` features. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # initialize the weight vector randomly\n",
    "    np.random.seed(42)\n",
    "    weight = np.random.rand(X.shape[1],1)    \n",
    "    # create a list to store the loss values \n",
    "    weights = []\n",
    "    losses = []\n",
    "     \n",
    "    for i in range(epochs):\n",
    "        # run the gradient step for the whole dataset\n",
    "        weight, MSE = gradient_step(X, y, weight, lrate)\n",
    "        # store the MSE loss of each batch of each epoch\n",
    "        weights.append(weight)\n",
    "        losses.append(MSE)\n",
    "                       \n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "347c5cbcd63ddad5bb58e49591c103e5",
     "grade": false,
     "grade_id": "cell-aaff04830999aeb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix X2 (n_samples, n_features):  (100, 4)\n",
      "Shape of label vector y2 (n_samples, 1):  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# generate a dataset for a regression problem\n",
    "# in this case we will set number of features to four\n",
    "\n",
    "X2, y2 = make_regression(n_samples=100, n_features=4, noise=20, random_state=42) \n",
    "y2 = y2.reshape(-1,1)\n",
    "\n",
    "X2 = preprocessing.scale(X2)\n",
    "\n",
    "print(\"Shape of feature matrix X2 (n_samples, n_features): \", X2.shape)\n",
    "print(\"Shape of label vector y2 (n_samples, 1): \", y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "539bf211db591288beb022cbef3fa961",
     "grade": false,
     "grade_id": "cell-1d16b774ab7239a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set epoch and learning rate\n",
    "epochs = 100\n",
    "lrate = 0.1\n",
    "\n",
    "# store results\n",
    "(weights, losses) = GD(X2, y2, epochs, lrate)\n",
    "\n",
    "# print the cost function\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "ax.set_ylabel('Loss', fontsize=16)\n",
    "ax.set_xlabel('epochs', fontsize=16)\n",
    "ax.plot(range(epochs), losses, 'b.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc2d77ca63346d8014e8d2896371cf4a",
     "grade": false,
     "grade_id": "cell-dfd4b62673e3f59b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>STUDENT TASK 2.4.</b> Compare your implementation with sklearn `LinearRegression` class. </h3>\n",
    "    \n",
    "We can compare the optimal weights learned by our simple algorithm and the optimal weight calculated by Sklearn's <a href=https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html>LinearRegression</a> class. This class does not use iterative gradient-based algorithms but rather calculate the optimal weight analytically <a href=\"https://www.youtube.com/watch?v=B-Ks01zR4HY\">learn more here</a>.\n",
    "\n",
    "Retrieve weight returned by `GD_onefeature()` function on the last epoch. Store it in variable `gd_weight`.\n",
    "Note, if weight returned by `GD_onefeature()` is similar to the optimal weight returned by sklearn `LinearRegression()` class. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b55b622569eafe340edf773d67adad7b",
     "grade": false,
     "grade_id": "cell-768f3b5086d9d2bc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal weights calculated by the LinearRegression model: [42.59035782]\n",
      "          Optimal weights calculated by the GD algorithm: [42.59035781]\n"
     ]
    }
   ],
   "source": [
    "# set epoch and learning rate\n",
    "epochs = 100\n",
    "lrate = 0.1\n",
    "\n",
    "# create a linear regression model \n",
    "reg = LinearRegression(fit_intercept=False) \n",
    "# fit the linear regression model\n",
    "reg.fit(x, y)\n",
    "# print the optimal coefficients\n",
    "print(f'Optimal weights calculated by the LinearRegression model: {reg.coef_.reshape(-1,)}')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# retrieve weight values returned by `GD_onefeature()` on the LAST epoch\n",
    "weights, _ = GD_onefeature(x,y,epochs,lrate)\n",
    "gd_weight = weights[-1]\n",
    "\n",
    "\n",
    "print(f'{\" \"*9} Optimal weights calculated by the GD algorithm: {gd_weight.reshape(-1,)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad9952ade102e5a533a5678620515b01",
     "grade": false,
     "grade_id": "cell-87e4e9c976e542b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "from numpy.testing import assert_almost_equal\n",
    "assert_almost_equal(reg.coef_.reshape(-1,), gd_weight.reshape(-1,), err_msg=\"reg.coef_ and gd_weight should be equal! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92cd84083c42f0989d931559f616eab1",
     "grade": true,
     "grade_id": "cell-d268e98b5be767f7",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61f30fbe346aece2ff663779dd2d4244",
     "grade": false,
     "grade_id": "cell-5b81fb9fdb99f93d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With correct implementation of `gradient_step_onefeature()` you should see that optimal weight value returned by our algorithm and `LinearRegression` are close (equal to 7 decimal places). Thus, our simple gradient descent algorithm does not deviate too much from the `Sklearn` implementation of linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f2fbae7fa1acf3ae888d1826ad5fa10",
     "grade": false,
     "grade_id": "cell-ab7d99814fb54024",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St5'></a>\n",
    "<div class=\" alert alert-success\">\n",
    "     <h3><b>STUDENT TASK 2.5. (OPTIONAL, not graded). </b> STOPPING CRITERIA. </h3>\n",
    "    \n",
    "Your task is to modify <code>GD_onefeature()</code> or <code>GD()</code> code and introduce stopping criteria other than a fixed number of iterations (epochs). You will need to compare loss values returned on the last two epochs and stop GD when the loss will stop decreasing significantly (e.g. the loss decrease is less than 1e-6). In addition, introduce a maximum number of iterations GD can perform, in order to prevent GD from running for a long time. The function should return last weight(s), loss, and the number of iterations performed.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69d102e2514604aae066731f73d0cf9f",
     "grade": false,
     "grade_id": "cell-38261cb510dfb699",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Hints</span>\n",
    "    </summary>\n",
    " <div class=\"summary-content\">\n",
    "     <ul>\n",
    " <li> You can use for-loop to iterate epoch times (maximum number of iterations) and if loop to check if the stopping condition(s) is true.</li>\n",
    " <li> You can use `math.isclose()` function to compare 2 values to desired precision.</li>\n",
    "     </ul>\n",
    "  </div>    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1412d734636f70bf9b15a4ca03dd99ae",
     "grade": false,
     "grade_id": "cell-7bd15f464106855b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def GD_stop(X, y, epochs, lrate):  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Function for performing gradient descent for linear predictor and MSE as loss function.\n",
    "    The helper function `gradient_step_onefeature` performs gradient step for dataset of size `m`, \n",
    "    where each datapoint has one feature. \n",
    "    \n",
    "    '''\n",
    "    # initialize weight vector randomly\n",
    "    np.random.seed(42)\n",
    "    weight = np.random.rand()   \n",
    "    weights, loss = [], []\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "    for iterations in range(epochs):\n",
    "        weight, mse = gradient_step_onefeature(X, y, weight, lrate)\n",
    "        weights.append(weight)\n",
    "        loss.append(mse)\n",
    "        if math.isclose(mse, 1e-6):\n",
    "            return weights, loss, iterations\n",
    "\n",
    "    return weights, loss, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29d037bc14a857c4fae50205af974cc6",
     "grade": false,
     "grade_id": "cell-4f78dace1c91a426",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 99\n",
      "Loss: 318.79310879490555\n",
      "Weights:[42.59035781]\n"
     ]
    }
   ],
   "source": [
    "# set learning rate\n",
    "lrate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "# store the results\n",
    "(weights, losses, iterations) = GD_stop(x, y, epochs, lrate)\n",
    "\n",
    "print(f'Number of epochs: {iterations}\\nLoss: {losses[-1]}\\nWeights:{weights[-1].reshape((-1, ))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b0ec695b1683f57710f47c14cf74e19",
     "grade": false,
     "grade_id": "cell-d1ebe1415256d4b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note, if algorithm stopped earlier than epochs set. Try to change tolerance parameters in `math.isclose()` and see if there is a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ab45c9bd1e77546c160e0dc931c6914",
     "grade": false,
     "grade_id": "cell-9a0211637fcab536",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"wrap-up\">\n",
    "    <div class=\"wrap-up-title\">Wrap up - Gradient Descent</div><br>\n",
    "    <div class=\"wrap-up-content\">\n",
    "        <ul>\n",
    "          <li>Gradient Descent (GD) is one of the most popular optimization methods used in the training of artificial neural networks.</li>\n",
    "          <li>GD is an iterative algorithm used to find optimal parameters (weights) of the model (ANN).</li>\n",
    "          <li>GD makes small steps in the opposite direction of the gradient, which is also the direction of the steepest descent.</li>\n",
    "          <li>Learning rate of GD will define how big are steps made in order to reach the minimum of the loss function.</li>\n",
    "          <li>If the learning rate is too small, it will take a long time for GD to learn optimal weights. If the learning rate is too high, GD parameters update will overshoot the minimum of the loss function and loss values will fluctuate around the minimum, or in the worst case, GD may diverge.</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae36849b7b8d2257c26a577a718cf432",
     "grade": false,
     "grade_id": "cell-9e201cd34a35a933",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Stochastic Gradient Descent \n",
    "\n",
    "Despite its conceptual simplicity, GD might not be practical in ML applications involving massive amounts of data. Consider image classification where state-of-the-art deep learning methods are trained on billions of images. The challenge in using GD for such big data applications is the computational complexity of computing the gradient $\\nabla f\\big( \\mathbf{w}^{(k)} \\big)$ of the loss function at the current estimate $\\mathbf{w}^{(k)}$. \n",
    "\n",
    "Let us have a closer look at the computation of the gradient for the special case of linear predictor maps $h^{(\\mathbf{w})}(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$. In this case, we can find a closed-form expression for the gradient: \n",
    "\n",
    "\\begin{align} \n",
    "\\nabla f\\big( \\mathbf{w}^{(k)} \\big)= - (2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big) \\big). \\end{align}\n",
    "\n",
    "The difficulty is that the summation involves all $m$ data points that form the training set. Thus, we might need to sum over billions of data points. Moreover, the data points might be stored decentralized all over the internet (in the \"cloud\"). A single iteration of GD might then simply take too long.\n",
    "\n",
    "In order to avoid the computational burden of computing the gradient, **stochastic GD (SGD)** approximates the gradient by using only a small subset (**batch**) of training data points. SGD is obtained from GD by replacing the exact gradient step with a noisy gradient update: \n",
    "\n",
    "$$\\mbox{(Noisy Gradient Step)} \\quad \\underbrace{\\mathbf{w}^{(k+1)}}_{\\mbox{new guess}} = \\underbrace{\\mathbf{w}^{(k)}}_{\\mbox{current guess}} - \\underbrace{\\alpha^{(k)}}_{\\mbox{step size}}\\mathbf{g}^{(k)} \\mbox{ with } \\mathbf{g}^{(k)} \\approx \\nabla f\\big(\\mathbf{w}^{(k)}\\big).$$ \n",
    "\n",
    "Note that we now use a varying step-size $\\alpha^{(k)}$ that changes along with the iterations. This is necessary in order to avoid accumulation of the gradient noise during the SGD updates. SGD methods typically decrease the learning rate $\\alpha^{(k)}$ as the iterations proceed. \n",
    "\n",
    "The most basic variant of SGD uses a single randomly chosen data point for computing the gradient estimate $\\mathbf{g}^{(k)}$. For the special case of linear predictor maps, we obtain \n",
    "$$ \\mathbf{g}^{(k)}   = -2 \\mathbf{x}^{(I)} \\big(y^{(I)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(I)} \\big) \\big).$$\n",
    "\n",
    "Note that the index $I$ of the data point is chosen randomly and independently for each new iteration $k$. Comparing this gradient estimate with the above formula for the exact gradient, we see that the SGD iteration does not require any summation over the training set. For large training sets, this might yield a significant reduction in computational requirements for SGD compared to GD. \n",
    "\n",
    "Plain SGD and GD can be interpreted as special cases of **mini-batch SGD**. Mini-batch SGD does not use a single randomly chosen data point to compute the gradient estimate but rather uses several randomly chosen data points that form a batch $\\mathcal{S} = \\big\\{ \\big(\\mathbf{x}^{(i_{1})},y^{(i_{1})} \\big),\\ldots,\\big(\\mathbf{x}^{(i_{S})},y^{(i_{S})} \\big) \\big\\}$ of size $s$. For linear predictor maps, the gradient estimate is computed using the batch via \n",
    "$$ \\mathbf{g}^{(k)}   = -(2/s) \\sum_{\\big(\\mathbf{x},y\\big) \\in \\mathcal{S}} \\mathbf{x} \\big(y - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x} \\big) \\big).$$\n",
    "\n",
    "Each iteration of mini-batch SGD uses a different batch of $s$ different data points. A sequence of iterations that uses each data point in one of the batches is referred to as one **epoch** (i.e. batches are disjoint). For example, if the training data set consists of $50$ data points and we use a batch size of $s=10$, then one epoch requires $50/10 = 5$ iterations of mini-batch SGD.  \n",
    "\n",
    "\n",
    "<img src=\"../../../coursedata/SGD/batch.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff9d236ebfa8a532c2a72289981bc65a",
     "grade": false,
     "grade_id": "cell-873d86a39fbcb451",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Important special cases of mini-batch SGD are obtained for certain choices of the **batch size**: \n",
    "\n",
    "- GD (also called batch GD or vanilla GD, batch size = size of dataset)\n",
    "- Mini-batch GD (1 < batch size < whole dataset)\n",
    "- SGD (batch size = 1)\n",
    "\n",
    "Note that the computational complexity of one iteration of mini-batch GD depends only on the batch size. Thus, \n",
    "for a fixed batch size (e.g. $s=128$) the complexity (runtime) of a single iteration becomes independent of the total number of training data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09ac08bcdabeba3d2916f0febc87850d",
     "grade": false,
     "grade_id": "cell-ec008639e2259829",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St6'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>STUDENT TASK 2.6.</b> STOCHASTIC GRADIENT DESCENT.</h3> \n",
    "    \n",
    "In this task, we modify our GD algorithm (function `GD_onefeature`) by providing and training the dataset in batches. You will need to complete <code>minibatchGD()</code> function, according to the instructions given below. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1bb3f0d0e4099c12a040c29ad0a124d",
     "grade": false,
     "grade_id": "cell-c2a00d5a26465f7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before implementing the SGD algorithm, we need to write a helper function `batch()` that divides the dataset into small batches. We will provide the feature vector or matrix `X` and label vector `y` as an input to the function. We also need to define the parameter `batch_size` which is the number of data points used for a single batch. The function `batch()` is a [Python generator function](https://docs.python.org/3/howto/functional.html#generators), meaning that the function can be used in for-loops and will return batches sequentially, one-by-one. Before splitting the dataset into batches, we will randomly shuffle the data. This ensures that every time we call `batch()` we obtain a batch having similar statistical properties. Loosely speaking, shuffling the dataset before creating the batches makes the individual data points independent and identically distributed (\"i.i.d.\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b42d0fd01a1ed4a94c8f0aab12fc31d7",
     "grade": false,
     "grade_id": "cell-76e1beb18772edb3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def batch(X,y,batch_size):\n",
    "    \n",
    "    '''\n",
    "    Function for creating mini-batches of the dataset.\n",
    "    The `yield` statement suspends the function’s execution and sends \n",
    "    a value back to the caller, but retains enough state to enable \n",
    "    function to resume where it is left off.  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # shuffle data points \n",
    "    # the permutation will randomly re-arrange the order of the numbers\n",
    "    # which will be used as indices to create X and y with data points in different order\n",
    "    np.random.seed(42) # for reproducibility, should NOT be used in real training\n",
    "    p = np.random.permutation(len(y))\n",
    "    X_perm = X[p] \n",
    "    y_perm = y[p]\n",
    "    \n",
    "    # generate batches\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X_perm[i:i + batch_size], y_perm[i:i + batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48ef3806d747f05b7f63118a3da2522c",
     "grade": false,
     "grade_id": "cell-a2a173cbe3bcbe36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"info\">\n",
    "    <div  class=\"info-title\"><i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>&nbsp; Info</div><br>\n",
    "    <div class=\"wrap-up-content\">\n",
    "        You can test Python generator by using <code>next()</code> function:\n",
    "    </div>\n",
    "    \n",
    "```python\n",
    ">>> gen =  Generator(args)\n",
    ">>> next(gen)\n",
    "```\n",
    "    \n",
    "    or for-loop:\n",
    "    \n",
    "```python   \n",
    ">>> for element in Generator(args):\n",
    "    print(element)    \n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b4041f7b02c0a8a0351fb2667b94ca9",
     "grade": false,
     "grade_id": "cell-1dc3b3cb1fc8cb8e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def minibatchGD(X, y, batch_size, epochs, lrate):  \n",
    "    \n",
    "    # initialize the weight randomly\n",
    "    np.random.seed(42)\n",
    "    weight = np.random.rand()  \n",
    "    # create a list to store the loss values \n",
    "    losses = []\n",
    "    weights = []\n",
    "     \n",
    "    for i in range(epochs):\n",
    "# YOUR CODE HERE\n",
    "        # Use another for-loop to iterate batch() generator and access batches one-by-one\n",
    "        # Feed  current batch to `gradient_step_onefeature()` and get weight and loss values\n",
    "        # Store current weight and loss values in corresponding lists\n",
    "        for element in batch(X,y,batch_size):\n",
    "            x, y = element[0], element[1]\n",
    "            weight, loss = gradient_step_onefeature(x,y,weight,lrate)\n",
    "            weights.append(weight)\n",
    "            losses.append(loss)\n",
    "      \n",
    "       # one epoch is finished when the algorithm goes through ALL batches\n",
    "    return weights, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75a52329cd635aaf4193c6e3eb71fa86",
     "grade": false,
     "grade_id": "cell-92dea7187bf6ce97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Hints</span>\n",
    "    </summary>\n",
    " <div class=\"summary-content\">\n",
    "     <a href=\"https://www.geeksforgeeks.org/generators-in-python/\">Here</a> you can find an example of using Python generator with for-loop. Use the same approach to iterate batch() generator.\n",
    "  </div>    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "847ccecd0055fb89848497f368786a20",
     "grade": false,
     "grade_id": "cell-fe1f1ab467887505",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/software/lib/python3.9/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "# len(weights) and len(loss) should be:\n",
    "# (number of samples/batch_size)*epochs - 100/50*2=4\n",
    "weights, loss = minibatchGD(x, y, 50, 2, 0.1)\n",
    "\n",
    "assert len(weights)==4, \"weights length is not correct!\"\n",
    "assert len(loss)==4, \"loss length is not correct!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e65f806675560127de78e68af2e2843a",
     "grade": true,
     "grade_id": "cell-a7b46020ff3a7ab3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a21ba9232c5953ba9195c3a6a7181307",
     "grade": true,
     "grade_id": "cell-7781c38310cc5b53",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe6b898b4e52823f64b132ae29d11b29",
     "grade": false,
     "grade_id": "cell-a14ca933f21b43e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St7'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>STUDENT TASK 1.7. </b> SGD - BATCH SIZE.</h3>\n",
    "    \n",
    "Try `minibatchGD()` function with different values of `batch_size` parameter and plot results.\n",
    "    \n",
    "Note - use feature vector **x** and label vector **y** as input data of  `minibatchGD()` function. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ce2dc28d91e0e3a96ccf3429447597c",
     "grade": false,
     "grade_id": "cell-caa8d0782861291e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's test our SGD implementation and run the algorithm for:\n",
    "\n",
    "- batch sizes = 1 (one data point) **(SGD)**\n",
    "- batch sizes = 10 **(mini-batch GD)**\n",
    "- batch sizes = 100 (entire dataset) **(Batch GD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6943871daaf23bb19a8400cc519448c3",
     "grade": false,
     "grade_id": "cell-d3601de090e616a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Hints</span>\n",
    "     </summary>\n",
    "     <div class=\"summary-content\">\n",
    "You can use for-loop to iterate list `batch_sizes` and pass these values to `minibatchGD()` function. \n",
    "    </div>\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06e94e186fb55c66ebd90da3df4cf9c4",
     "grade": false,
     "grade_id": "cell-5dbba44eba46a098",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set epoches and learning rate\n",
    "epochs = 100\n",
    "lrate = 0.02\n",
    "\n",
    "\n",
    "batch_sizes = [1, 10, 100]\n",
    "# list for storing weights and loss for each batch size (length of both lists=3)\n",
    "# we will use these lists for plotting\n",
    "weights_batches = []\n",
    "loss_batches = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# iterate `batch_sizes` list\n",
    "for batch_size in batch_sizes:\n",
    "#     compute weights & losses for a given batch size value\n",
    "#     append weights & losses to lists weights_batches & loss_batches\n",
    "    weights, loss = minibatchGD(x, y, batch_size, epochs, lrate)\n",
    "    weights_batches.append(weights)\n",
    "    loss_batches.append(loss_batches)\n",
    "    \n",
    "\n",
    "\n",
    "for batch_size, weights, loss in zip(batch_sizes, weights_batches,loss_batches):\n",
    "    plt.plot(weights, loss, label=\"batch size\"+str(batch_size))\n",
    "    plt.legend()\n",
    "\n",
    "plt.xlabel(\"weight\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e524bda361d846742216e0336ba95f57",
     "grade": true,
     "grade_id": "cell-fa4f330c2cb1e60e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "assert len(weights_batches)==len(batch_sizes), \"weights_batches length is not correct!\"\n",
    "assert len(loss_batches)==len(batch_sizes), \"loss_batches length is not correct!\"\n",
    "# below length computed as (number of samples/batch_size)*epochs\n",
    "assert len(loss_batches[0])==10000, \"loss_batches[0] length should be (100/1)*100=10000!\"\n",
    "assert len(loss_batches[1])==1000, \"loss_batches[1] length should be (100/10)*100=1000!\"\n",
    "assert len(loss_batches[2])==100, \"loss_batches[2] length should be (100/100)*100=100!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55a5a6e50a12b8d8cb3e349b9a4785f0",
     "grade": true,
     "grade_id": "cell-41c59e0f0f2e20fe",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebf1c116ab85066770d8762d9742af9a",
     "grade": false,
     "grade_id": "cell-ea33d721e419dbee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that the deviations are larger for smaller batch sizes. This makes sense, as the gradient estimates are more accurate when we use more data points in a batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a87189b078ee14f3dc4d05c01db23be3",
     "grade": false,
     "grade_id": "cell-ba8e5ba8db59646d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's use the loss values for each batch size stored in the list `loss_batches` for plotting and will print out learned weights stored in `weights_batches` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "717967b3bd6f7638cf0504bcca8632ab",
     "grade": false,
     "grade_id": "cell-f507efc6bc25907f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# history of the MSE loss inccured during learning\n",
    "batch_size1_loss   = loss_batches[0]\n",
    "batch_size10_loss  = loss_batches[1]\n",
    "batch_size100_loss = loss_batches[2]\n",
    "\n",
    "# let's check that the length of list `loss` is equal to\n",
    "# x.shape[0]/batch_size*epochs\n",
    "\n",
    "print(f\"Total number of iterations = (sample size/batch size)*epochs\")\n",
    "print(f\"\\nEpochs: {epochs}\")\n",
    "print(f\"Sample size: {x.shape[0]}\")\n",
    "print(f\"Batch sizes: 1, 10, 100\")\n",
    "print(f\"Iterations per epoch: {x.shape[0]/1.0:.0f}, {x.shape[0]/10.0:.0f}, {x.shape[0]/100.0:.0f}\")\n",
    "print(f\"Total number of iterations: {len(batch_size1_loss)}, {len(batch_size10_loss)}, {len(batch_size100_loss)}\")\n",
    "\n",
    "# display weights learnt during the SGD\n",
    "print(f\"\\nWeights:\\n\\nSGD with batch size = 1 results in weight w = {weights_batches[0][-1]:.2f}\\\n",
    "                 \\nSGD with batch size = 10 results in weight w = {weights_batches[1][-1]:.2f}\\\n",
    "                 \\nSGD with batch size = 100 results in weight w = {weights_batches[2][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86f0468801ddd9e243ee942ed1f3a2ea",
     "grade": false,
     "grade_id": "cell-197719c66c1ae6fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The results of SGD with different batch sizes should be pretty close. Let's compare our SGD implementation with sklearn [`SGDRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) class implementation. In `SGDRegressor` implementation, the gradient of the loss is estimated each sample at a time (batch size=1 in other words) and the model is updated along the way with a decreasing learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8f4368b4ee9b7cc075248e7e7b4820e",
     "grade": false,
     "grade_id": "cell-073a69e706d17886",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "reg = SGDRegressor(fit_intercept = False, max_iter=100, tol=1e-3)\n",
    "reg.fit(x,y.reshape(-1,))\n",
    "\n",
    "reg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bcff507a92616b758ec17b1eaa2e350",
     "grade": false,
     "grade_id": "cell-97f489247f04856f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Thus, our simple SGD algorithm does not deviate too much from the `sklearn` implementation. \\\n",
    "Let's plot the loss values for the first 100 iterations incurred during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cce26fe7fd6890f027ce7202b6d7effe",
     "grade": false,
     "grade_id": "cell-e755ac6d58a80022",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the figure and axes objects\n",
    "# there will be 3 subplots in one row, the y-axis is shared between subplots\n",
    "fig, axes = plt.subplots(1,3, sharey=True, figsize=(15,5))\n",
    "\n",
    "# create lists of loss values and batch sizes for further iteration in for-loop\n",
    "batch_loss_list = [batch_size1_loss, batch_size10_loss, batch_size100_loss]\n",
    "batch_size      = [1,10,100] \n",
    "\n",
    "for ax, batch_loss, size in zip(axes, batch_loss_list, batch_size):\n",
    "    # plot only first 100 values\n",
    "    ax.plot(np.arange(len(batch_loss[:100])), batch_loss[:100])\n",
    "    # remove top and right subplot's frames \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    # set subplot's title\n",
    "    ax.set_title(\"batch size = \"+str(size), fontsize=18)\n",
    "\n",
    "# set x- and y-axis labels\n",
    "axes[0].set_xlabel('batch #', fontsize=18)\n",
    "axes[0].set_ylabel('Loss', fontsize=18)\n",
    "\n",
    "# display figure\n",
    "plt.ylim(0,10000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eafa8fdbae52a06aa8ccb448cb776efb",
     "grade": false,
     "grade_id": "cell-9b626daecc63f8bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The above figure indicates that when using SGD with a small batch size, the loss is not decreasing monotonically but somewhat randomly fluctuating around a long-term decreasing trend. This happens because the weight updates use \"noisy\" estimates of the gradient. The noisy estimate is calculated by an averaging process using the data points in the mini-batch. The smaller the mini-batch size, the fewer data points we use for computing the average. Thus, the gradient noise becomes stronger with smaller batch size. \n",
    "\n",
    "**Note!** In our exercises we used a constant learning rate. In order to avoid the accumulation of the gradient noise while running SGD updates, the learning rate needs to be gradually decreased, e.g. by using diminishing learning rate, thus resulting in a smoother learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e017703c8fb2e3f445fbc448c448f746",
     "grade": false,
     "grade_id": "cell-2a6b165617a3a4b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below are the animations illustrating the training process with SGD where the batch size is all data points (upper panel) and where the batch size is 10 data points (lower panel, the current batch marked with red color). In line with the loss plots we created above, mini-batch SGD is noisier than batch GD. Although it seems that using plain batch GD is a faster way to reach the minimum of the loss function, in practice when working with large datasets and thousands of parameters (weights) in neural networks, this approach will be slower and more computationally expensive than mini-batch SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "757af3f302c4854f5f44bb4d5c33b5e6",
     "grade": false,
     "grade_id": "cell-2032e57953d2d15e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Illustration of SGD iterations for batch size = 100 (batch covers entire dataset)** \n",
    "\n",
    "<img src=\"../../../coursedata/SGD/minibatchGD1.gif\" width=\"750\" align=\"center\">\n",
    "\n",
    "**Illustration of SGD iterations for batch size = 10 datapoints**\n",
    "\n",
    "<img src=\"../../../coursedata/SGD/minibatchGD2.gif\" width=\"750\" align=\"center\">\n",
    "\n",
    "During each iteration of SGD, 10 data points are randomly selected to constitute a batch. This batch is used to compute the gradient estimate. The data points in the batch are shown in red. Note that during each iteration, a different set of 10 data points is chosen for the batch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c1b329db94545c0f0bc0249f93f18c2",
     "grade": false,
     "grade_id": "cell-1469b9fd7044cbe6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"wrap-up\">\n",
    "    <div class=\"wrap-up-title\">Wrap up - Stochastic Gradient Descent</div><br>\n",
    "    <div class=\"wrap-up-content\">\n",
    "        <ul>\n",
    "          <li>Vanilla or batch GD computes the gradient of the loss function w.r.t. to the parameters (weights) for the entire training dataset. Therefore, to perform just one update, the gradient of the whole dataset needs to be computed, which makes batch GD very slow.</li>\n",
    "          <li>SGD, in contrast, computes the gradient of the loss function and performs updates for each data point.</li>\n",
    "          <li>SGD performs faster parameter updates, but suffers from high variance, especially with a constant learning rate.</li>\n",
    "          <li>Mini-batch GD computes the gradient of the loss function for every batch of the dataset, thus combines the benefits of both, batch GD and SGD. It is faster than batch GD but prone to less variation in parameters up-dates than SGD.</li>\n",
    "          </ul>\n",
    "          <br>\n",
    "        Note, that in deep learning literature mini-batch GD sometimes is also called SGD.\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d8e8720a840f036b21b60b0bef35408",
     "grade": false,
     "grade_id": "cell-5ed712c900639704",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Variants of Gradient-Based Optimization Algorithms\n",
    "\n",
    "Besides plain GD or mini-batch SGD, many deep learning methods use somewhat more advanced variants of gradient-based algorithms or optimizers ([list of optimizers available in deep learning Python library Keras](https://keras.io/api/optimizers/)). Some of the most known are SGD with momentum, RMSprop, and Adam. \n",
    "\n",
    "Much like GD and SGD, these algorithms use gradients of the loss function $f(\\mathbf{w})$ to find weights $\\mathbf{w}$ such that the predictor $h^{(\\mathbf{w})}$ achieves (nearly) minimum loss. These variants differ in how they use (or \"interpret\") the gradient information to find the fastest route towards the minimum. In some cases, these variants can find good weight vectors significantly faster (using fewer iterations) compared to mini-batch SGD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbc42fc18f682042ee201189b03f5fc7",
     "grade": false,
     "grade_id": "cell-5d0495b1535d7fee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Gradient Descent with momentum\n",
    "\n",
    "One simple improvement of gradient descent is adding a \"momentum\" term to the weight update equation. GD manages poorly when the loss surface is much steeper in one dimension than in the other, e.g. in ravines of the loss function. You can see on the animation below loss \"landscape\": weights values are shown on x and y axes and loss values are shown on the z-axis. GD (black trace) bounces back and force in the narrow symmetric ravine, unable to converge to the minimum of the loss function.\n",
    "\n",
    "The addition of the \"momentum\" helps to damp oscillations and accelerate in the relevant direction. The common analogy is the ball moving in a bowl and gaining speed and momentum in the downhill direction, rather than moving to the left or right on the sides of the bowl.\\\n",
    "GD with momentum is implemented by storing the information about past gradients in a vector ${v}_{t}=\\beta*{v}_{t-1} + \\alpha*\\nabla f$.\\\n",
    "The weight is updated as follows:\n",
    "${w}_{t}={w}_{t-1}-{v}_{t}$\n",
    "\n",
    "Momentum will increase for dimensions where gradients will point in the same direction and will decrease for dimensions where gradients change direction (oscillates). You can see on the animation below, how GD with momentum (red trace) decreases \"zigzagging\" amplitude and moves faster in the direction of the minimum of the loss function. Note that it also overshoots the minimum, but \"corrects\" itself and moves back converging to the minimum. Setting $\\beta$ to a lower value would decrease momentum and would prevent overshooting. \n",
    "\n",
    "<img src=\"../../../coursedata/SGD/GDMomentum2.gif\" width=\"700\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7768723172d867f7e1a57a3b837f608",
     "grade": false,
     "grade_id": "cell-62eb6d4e5d148347",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below you can see how GD with momentum (red trace) gained momentum after the first weights update, escaped the ravine, and \"hopped\" over \"hill\" (the area where loss values are higher). In this case, it may be not an optimal path to the minimum, but in the more complex landscapes momentum can help to escape the local minimum given big enough momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf42a2dfde1b6dedb5744fef153ed001",
     "grade": false,
     "grade_id": "cell-e7d5abd78e65e01b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"../../../coursedata/SGD/GDMomentumRosenbrock2.gif\" width=\"750\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57ec15027232e7e7ca98980754017acc",
     "grade": false,
     "grade_id": "cell-2cc4fbad06a334ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Other variants of GD improve different aspects of the algorithm. Nesterov accelerated gradient is basically a modification of the GD with momentum. Adagrad and its extension Adadelta uses different learning rates for parameters, which is useful when dealing with sparse data. RMSprop and Adam are currently a popular choice of GD-based variants with an adaptive learning rate ([paper about GD variants](https://arxiv.org/pdf/1609.04747.pdf)). \n",
    "\n",
    "The animation below compares the \"routes\" taken by several optimizers to find a minimum of the [six-hump camel](https://www.sfu.ca/~ssurjano/camel6.html) function. As you can see different GD variants may take different paths, which can be controlled by hyperparameters tuning (such as learning rate and coefficients like $\\beta$ in GD with momentum).\n",
    "\n",
    "<img src=\"../../../coursedata/SGD/camel3D.gif\" width=\"750\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c53713b3eeb7452f2f659a172bdba2c3",
     "grade": false,
     "grade_id": "cell-4db4e564f64fd90f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Conclusion \n",
    "\n",
    "We have discussed the basic idea of using gradients of loss functions to iteratively improve the parameter values (weights) in a predictor map. Gradient-based methods such as SGD and its variants turn out to be the perfect tool for training deep neural networks in several aspects. First, somewhat surprisingly, SGD quickly finds weights for an ANN such that it performs well on new data points which are different from the training data. Moreover, mini-batch SGD requires only to have enough working memory (\"RAM\") to store the current batch (subset) of training data points instead of the entire dataset (which might be billions of high-resolution images). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75eade9fbe283663fb9c57279a3bd802",
     "grade": false,
     "grade_id": "cell-d67f67c973361771",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c61e48728ffa57e6a7abae5b3c46b60c",
     "grade": false,
     "grade_id": "cell-826b0d68236d3ec1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.1.</h3>\n",
    "\n",
    "What is the role of a loss function within machine learning methods?\n",
    "         \n",
    "Select one:\n",
    "\n",
    "1. To reduce the memory requirements of a machine learning method.\n",
    "\n",
    "2. To transform the data into a computer-friendly format.\n",
    "\n",
    "3. To speed up the learning process of an ML method.\n",
    "\n",
    "4. To measure the quality of a particular predictor map.\n",
    "         \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cabe9dcc5d4d239fec8fd1f17555127e",
     "grade": false,
     "grade_id": "cell-1d23029c8fc0c549",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "answer_21  = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c07babf672b6f87ef019d110057abf77",
     "grade": true,
     "grade_id": "cell-1c14f10dcabd0e6d",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_21 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "448960a938a70b58e493155a4a4fa6d8",
     "grade": false,
     "grade_id": "cell-173eaa264e18d0d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.2.</b></h3>\n",
    "         \n",
    "Consider the average loss, or training loss, incurred by a predictor map on a set of labeled data points (the training set). The predictor map involves some adjustable weights. Which of the following statements is correct?\n",
    "Select one:\n",
    "\n",
    "1. The training loss (average loss incurred on training data points) depends only on the labels.\n",
    "\n",
    "2. The training loss does not depend on the weights of the predictor map.\n",
    "\n",
    "3. The training loss does not depend on the features of the data points in the training set.\n",
    "\n",
    "4. The training loss depends on the features, labels of training data points as well as on weights of the predictor map.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17d4d36d582e39d6bca49a932d6967c9",
     "grade": false,
     "grade_id": "cell-3f3a40ae966607fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "answer_22  = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50676bc53b010f1eb1a97ae466bac54f",
     "grade": true,
     "grade_id": "cell-4ba09434dbdc1355",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_22 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "930b44192bfdaa805860429c46640efc",
     "grade": false,
     "grade_id": "cell-78633828d2da2eaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.3.</h3>\n",
    "\n",
    "Which one of the following completions of the sentence below is correct? Gradient descent is a ...\n",
    "\n",
    "1. method to visualize data points.\n",
    "\n",
    "2. iterative algorithm for finding a good choice for the weights (parameters) of a predictor map.\n",
    "\n",
    "3. method to avoid overfitting.\n",
    "\n",
    "4. method to divide the data into batches.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31ee0484af6761f26bdaf645acc2d108",
     "grade": false,
     "grade_id": "cell-1419b5a420cfa16a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "answer_23  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "205cccf8a0b95e591cab55a51faf64d7",
     "grade": true,
     "grade_id": "cell-26f1ff9837b88b9f",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_23 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a946568d047b797dcd22fe8b3a1c0542",
     "grade": false,
     "grade_id": "cell-386c530d9dbd20df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.4.</b></h3>\n",
    "\n",
    "Consider a single GD step (update) $\\mathbf{w}^{(k+1)}=\\mathbf{w}^{(k)}−\\alpha \\nabla f(\\mathbf{w}^{(k)})$ using the training loss $f(\\mathbf{w})$ incurred by an ANN with parameters $\\mathbf{w}$ on $m$ labeled training data points $(\\mathbf{x}^{(1)},{y}^{(1)}),…,(\\mathbf{x}^{(m)},{y}^{(m)})$. The function $f(\\mathbf{w})$ is known to be differentiable and convex. The GD update includes an adjustable (tunable) parameter $\\alpha>0$ which is referred to as step size or learning rate. Which of the following statements is correct?\n",
    "\n",
    "Select one:\n",
    "\n",
    "1. The GD iterates $\\mathbf{w}^{(0)},\\mathbf{w}^{(1)},\\mathbf{w}^{(2)},…$ converge always, no matter what our choice is for $\\alpha$.\n",
    "\n",
    "2. The GD iterates can only converge if the feature vectors $\\mathbf{x}^{(1)},…,\\mathbf{x}^{(m)}$ have a norm smaller than one. \n",
    "\n",
    "3. If we choose too **small** learning rate $\\alpha$, GD iterates will converge anyway (assuming infinite time and computational resources).\n",
    "\n",
    "4. If we choose too **large** learning rate $\\alpha$, GD iterates will converge anyway (assuming infinite time and computational resources).   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28aa60d2a5e98f1ba9fb8219715b205c",
     "grade": false,
     "grade_id": "cell-86eff0cd2fb8da77",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "answer_24  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "328e7c3a9f4aad646226b86d23c69ca2",
     "grade": true,
     "grade_id": "cell-3ce60ca04b6476c3",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_24 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24da4008db49ab0e430b5836e440854f",
     "grade": false,
     "grade_id": "cell-07b1778475a2511d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.5.</h3>\n",
    "\n",
    "Pick the correct completion for the following sentence. \n",
    "\n",
    "In contrast to plain gradient descent (GD), stochastic gradient descent (SGD) ...\n",
    "\n",
    "\n",
    "1. is guaranteed to strictly decrease the training error after each iteration.\n",
    "\n",
    "2. always learns ANN parameters resulting in smaller training errors than the parameters learnt by GD.\n",
    "\n",
    "3. can only be used for ANNs with more than 100 layers.  \n",
    "\n",
    "4. typically requires less computation (time).   \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdfa0df9c809153c644599178238cfd8",
     "grade": false,
     "grade_id": "cell-874a654a73538007",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "answer_25  = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1099f3c45607a5cec4f5b7519ecdc74e",
     "grade": true,
     "grade_id": "cell-3192673c0457a226",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_25 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68c1b5035dc45444806f1101cfbdd092",
     "grade": false,
     "grade_id": "cell-2f38143a4eee875b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.6.</h3>\n",
    "\n",
    "Which one of the following statements is correct?\n",
    "\n",
    "1. The batch size in SGD has no influence on the prediction accuracy obtained with the learnt ANN parameters. \n",
    "\n",
    "2. A single iteration of gradient descent updates the parameter vector into the opposite direction of the gradient of the loss function.\n",
    "\n",
    "3. A single iteration of gradient descent updates the parameter vector into the direction of the gradient of the loss function.\n",
    "\n",
    "4. In the context of SGD, one epoch refers to one single update of the ANN parameters based on a noisy gradient estimate computed over a batch.   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f06ec23ed37b63691b36a7f3d435918",
     "grade": false,
     "grade_id": "cell-f197d4a928f590ae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "answer_26  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95d2add5467f7b5d622335e552206be3",
     "grade": true,
     "grade_id": "cell-25e60f29e9109354",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_26 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d5b03db5a9a8a9e3da1211f36c56607",
     "grade": false,
     "grade_id": "cell-10d7b11165411e7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "     <h3><b>Question 2.7.</h3>\n",
    "    \n",
    "Consider a dataset consisting of 4800 data points $({x}^{(1)},{y}^{(1)}),…,({x}^{(4800)},{y}^{(4800)})$.\n",
    "If we run mini-batch SGD with a batch size of 64 and for 100 epochs, what is the total number of iterations used in SGD? \n",
    "         \n",
    "Select one option:\n",
    "\n",
    "1. 30\n",
    "\n",
    "2. 160000\n",
    "\n",
    "3. 15\n",
    "\n",
    "4. 7500\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e523b7143dde230214b842009a03155a",
     "grade": false,
     "grade_id": "cell-b24bba98b125d531",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "answer_27  = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "979bc1ddbdf94d66db09a7f03fd492fb",
     "grade": true,
     "grade_id": "cell-d8317ef591c65e02",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_27 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 2,
           "op": "addrange",
           "valuelist": "8"
          },
          {
           "key": 2,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.136017px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
